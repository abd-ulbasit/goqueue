<p align="center">
  <img src="docs/assets/logo.svg" alt="GoQueue Logo" width="180">
</p>

<h1 align="center">GoQueue</h1>

<p align="center">
  <strong>The Message Queue That Doesn't Make You Choose</strong><br>
  <em>Kafka's durability + SQS's simplicity + Features neither has</em>
</p>

<p align="center">
  <a href="#the-problem">The Problem</a> â€¢
  <a href="#the-solution">The Solution</a> â€¢
  <a href="#unique-features">Unique Features</a> â€¢
  <a href="#quick-start">Quick Start</a> â€¢
  <a href="#comparison">Comparison</a> â€¢
  <a href="#architecture">Architecture</a>
</p>

---

## The Problem

Every distributed message queue forces painful trade-offs:

### ğŸ˜¤ Kafka Frustrations
| Pain Point | Impact |
|------------|--------|
| **Stop-the-world rebalancing** | All consumers pause when one joins/leaves. 30+ second latency spikes. |
| **No per-message acknowledgment** | Must commit offsets in batches. One poison message blocks the partition. |
| **No native delay/scheduled messages** | Need Kafka Streams, external scheduler, or hack with partitions. |
| **Operational complexity** | ZooKeeper (legacy) or KRaft. Cluster management is a full-time job. |
| **No built-in DLQ** | Have to build your own dead letter handling. |

### ğŸ˜¤ SQS Frustrations
| Pain Point | Impact |
|------------|--------|
| **No ordering guarantees** | FIFO queues have 300 msg/s limit. Standard queues = chaos. |
| **No replay capability** | Message gone after processing. Can't reprocess historical data. |
| **No consumer groups** | Every consumer sees every message or you manage routing yourself. |
| **Polling only** | No push model. Paying for empty receives. |
| **Vendor lock-in** | Your queue logic married to AWS forever. |

### ğŸ˜¤ RabbitMQ Frustrations
| Pain Point | Impact |
|------------|--------|
| **Not designed for log streaming** | No offset replay, no partitions, memory-bound queues. |
| **Complex routing overkill** | Exchange/binding/queue model when you just want pub/sub. |
| **No horizontal consumer scaling** | Competing consumers, but no partition-based parallelism. |

---

## The Solution

**GoQueue** combines the best of all worlds:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           GoQueue                                   â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚   Kafka's Log       â”‚ +  â”‚   SQS's Simplicity  â”‚                 â”‚
â”‚  â”‚                     â”‚    â”‚                     â”‚                 â”‚
â”‚  â”‚ â€¢ Append-only log   â”‚    â”‚ â€¢ Per-message ACK   â”‚                 â”‚
â”‚  â”‚ â€¢ Partition scaling â”‚    â”‚ â€¢ Visibility timeoutâ”‚                 â”‚
â”‚  â”‚ â€¢ Offset replay     â”‚    â”‚ â€¢ Dead letter queue â”‚                 â”‚
â”‚  â”‚ â€¢ Consumer groups   â”‚    â”‚ â€¢ Simple API        â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                                     â”‚
â”‚                    + Features Neither Has                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ â€¢ Zero-downtime rebalancing (incremental, not stop-world)   â”‚    â”‚
â”‚  â”‚ â€¢ Native delay/scheduled messages (built-in timer wheel)    â”‚    â”‚
â”‚  â”‚ â€¢ Message-level tracing (track any message's journey)       â”‚    â”‚
â”‚  â”‚ â€¢ Point-in-time replay (not just offset, but timestamp)     â”‚    â”‚
â”‚  â”‚ â€¢ Priority lanes (fast-track within same topic)             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Unique Features

### 1. ğŸ”„ Zero-Downtime Rebalancing

**The Problem:** Kafka's rebalancing protocol stops ALL consumers in a group, even if only one partition is affected. This causes latency spikes and message processing delays.

**GoQueue's Solution:** Cooperative incremental rebalancing. Only affected partitions pause. Other consumers continue processing uninterrupted.

```
Kafka Rebalance (Stop-the-World):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Consumer A: [P0, P1, P2] â”€â”€STOPâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º [P0, P1] â”€â”€RESUMEâ”€â”€â–º
Consumer B: (joining)    â”€â”€WAITâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º [P2]     â”€â”€STARTâ”€â”€â”€â–º
                              â–²                    â–²
                              â””â”€â”€ ALL STOPPED â”€â”€â”€â”€â”€â”˜
                                  (30+ seconds)

GoQueue Rebalance (Cooperative):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Consumer A: [P0, P1, P2] â”€â”€â”€â”€â”€â”€â”€[P0, P1]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
                         (P2 handoff)  â†˜
Consumer B: (joining)    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[P2]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
                                          â–²
                                          â””â”€â”€ Only P2 paused briefly
                                              (< 1 second)
```

**Interview Talking Point:**
> "Kafka's rebalancing is the #1 operational pain point I've seen. Even Confluent added incremental cooperative rebalancing in 2.4, but it's opt-in and complex. I built it as the default behavior."

---

### 2. â° Native Delay & Scheduled Messages

**The Problem:** Kafka has no built-in way to delay message delivery. Common workarounds:
- Kafka Streams with windowing (complex)
- External scheduler polling the queue (inefficient)
- Multiple topics with naming conventions (hacky)

**GoQueue's Solution:** First-class delay support with hierarchical timer wheel.

```go
// Publish with delay
client.Publish("orders", &Message{
    Key:   "order-123",
    Value: orderJSON,
    Delay: 30 * time.Minute,  // Deliver in 30 minutes
})

// Publish at specific time
client.Publish("reminders", &Message{
    Key:       "user-456",
    Value:     reminderJSON,
    DeliverAt: time.Date(2025, 12, 25, 9, 0, 0, 0, time.UTC),
})
```

**Use Cases:**
- Order cancellation after timeout
- Retry with exponential backoff
- Scheduled notifications
- Saga pattern timeouts
- Rate limiting with delay buckets

**Implementation:** Hierarchical timing wheel (O(1) insert/delete) + persistent delay index

**Interview Talking Point:**
> "Timer wheels are used in Linux kernel, Netty, and Kafka's purgatory. I implemented the hierarchical variant for O(1) operations across a wide time range - from milliseconds to days."

---

### 3. ğŸ¯ Message-Level ACKs with Log Durability

**The Problem:**
- **Kafka:** Batch offset commit. One slow/failing message blocks entire partition.
- **SQS:** Per-message ACK, but no replay. Message is gone after deletion.

**GoQueue's Solution:** Both! Per-message acknowledgment with visibility timeout, but messages stay in the log for replay.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Message Lifecycle                             â”‚
â”‚                                                                      â”‚
â”‚  Published â”€â”€â–º Visible â”€â”€â–º In-Flight â”€â”€â–º Acknowledged                â”‚
â”‚                  â”‚             â”‚              â”‚                      â”‚
â”‚                  â”‚             â”‚              â””â”€â”€â–º Still in log!     â”‚
â”‚                  â”‚             â”‚                   (replay possible) â”‚
â”‚                  â”‚             â”‚                                     â”‚
â”‚                  â”‚             â””â”€â”€â–º Visibility timeout expires       â”‚
â”‚                  â”‚                  â†“                                â”‚
â”‚                  â”‚             Redelivered (retry_count++)           â”‚
â”‚                  â”‚                  â†“                                â”‚
â”‚                  â”‚             max_retries exceeded                  â”‚
â”‚                  â”‚                  â†“                                â”‚
â”‚                  â”‚             Sent to DLQ                           â”‚
â”‚                  â”‚                                                   â”‚
â”‚                  â””â”€â”€â–º Can replay from any offset or timestamp!       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Interview Talking Point:**
> "SQS users love per-message ACKs. Kafka users love replay. I asked: why not both? The log stores everything, but a visibility index tracks what's currently being processed."

---

### 4. ğŸ” Built-in Message Tracing

**The Problem:** "Where did my message go?" requires external tracing systems (Jaeger, Zipkin) and careful instrumentation.

**GoQueue's Solution:** Every message has a lifecycle tracked internally.

```bash
$ goqueue-cli trace msg-abc123

Message: msg-abc123
Topic: orders
Partition: 2
Offset: 45678

Timeline:
â”œâ”€ 2025-01-15T10:30:00.000Z  PUBLISHED     producer=order-service
â”œâ”€ 2025-01-15T10:30:00.005Z  REPLICATED    nodes=[node-1, node-2]
â”œâ”€ 2025-01-15T10:30:00.123Z  DELIVERED     consumer=processor-1
â”œâ”€ 2025-01-15T10:30:30.456Z  REDELIVERED   reason=visibility_timeout
â”œâ”€ 2025-01-15T10:30:35.789Z  DELIVERED     consumer=processor-2
â”œâ”€ 2025-01-15T10:30:36.012Z  ACKNOWLEDGED  consumer=processor-2
â””â”€ 2025-01-15T10:30:36.015Z  RETAINED      expires=2025-01-22T10:30:00Z

Delivery Attempts: 2
Total Latency: 36.012s (publish to final ack)
```

**Interview Talking Point:**
> "Observability is usually bolted on. I made it intrinsic. Every message carries its history, queryable without external tools."

---

### 5. âª Point-in-Time Replay

**The Problem:** Kafka can replay from an offset, but:
- What offset corresponds to "yesterday at 3pm"?
- What if you need messages that were already deleted by retention?

**GoQueue's Solution:** Timestamp-indexed replay with optional snapshot retention.

```bash
# Replay all messages from a specific time
$ goqueue-cli consume orders --group replay-test --from-time "2025-01-14T15:00:00Z"

# Replay to a specific time (stop there)
$ goqueue-cli consume orders --group replay-test --from-time "2025-01-14T15:00:00Z" --to-time "2025-01-14T16:00:00Z"

# Replay even deleted messages (if snapshots enabled)
$ goqueue-cli consume orders --group audit --from-snapshot "2025-01-01" --include-deleted
```

**Interview Talking Point:**
> "Event sourcing needs deterministic replay. Kafka gives you offsets, but mapping timestamps to offsets is manual. I built a time-indexed log that makes point-in-time queries natural."

---

### 6. ğŸš€ Priority Lanes

**The Problem:** Important messages stuck behind bulk messages. Solutions:
- Separate topics (routing complexity)
- Partition hacks (lose ordering)

**GoQueue's Solution:** Lanes within partitions. Same ordering guarantees, but priority delivery.

```go
// High-priority payment notification
client.Publish("notifications", &Message{
    Key:      "user-123",
    Value:    paymentJSON,
    Priority: PriorityHigh,  // Delivered first
})

// Low-priority marketing email
client.Publish("notifications", &Message{
    Key:      "user-123",
    Value:    marketingJSON,
    Priority: PriorityLow,   // Delivered when high lane empty
})
```

**Within same partition**, messages from the same key maintain order, but high-priority messages are delivered before low-priority ones at partition level.

---

## Comparison

| Feature | GoQueue | Kafka | SQS | RabbitMQ |
|---------|---------|-------|-----|----------|
| **Append-only log** | âœ… | âœ… | âŒ | âŒ |
| **Offset replay** | âœ… | âœ… | âŒ | âŒ |
| **Consumer groups** | âœ… | âœ… | âŒ | Partial |
| **Partition scaling** | âœ… | âœ… | âŒ | âŒ |
| **Per-message ACK** | âœ… | âŒ | âœ… | âœ… |
| **Visibility timeout** | âœ… | âŒ | âœ… | âœ… |
| **Built-in DLQ** | âœ… | âŒ | âœ… | âœ… |
| **Native delay messages** | âœ… | âŒ | âŒ | âœ… (plugin) |
| **Zero-downtime rebalance** | âœ… | Partial | N/A | N/A |
| **Message tracing** | âœ… | âŒ | âŒ | âŒ |
| **Point-in-time replay** | âœ… | âŒ | âŒ | âŒ |
| **Priority lanes** | âœ… | âŒ | âŒ | âœ… |
| **No external deps** | âœ… | âŒ (ZK/KRaft) | N/A | âŒ (Erlang) |

---

## Performance

### Cluster Benchmark Results

Benchmarks run from **within** a 3-node EKS cluster (c5.xlarge instances, 4 vCPU, 8GB RAM each):

| Mode | Configuration | Throughput |
|------|--------------|------------|
| **Sequential** | Single message at a time | **~320 msgs/sec** |
| **Concurrent** | 8 parallel threads | **~1,300 msgs/sec** |
| **Batch** | 100 msgs/batch | **~30,000 msgs/sec** |
| **Large Batch** | 1000 msgs/batch | **~220,000 msgs/sec** |

### Scaling with Batch Size

```
Batch Size    Throughput       Improvement
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     1        ~320/s           baseline
    10        ~3,000/s         ~10x
    50        ~15,000/s        ~50x
   100        ~30,000/s        ~100x
   200        ~60,000/s        ~200x
   500        ~130,000/s       ~400x
  1000        ~220,000/s       ~700x
```

**Key Insight**: Batch publishing amortizes per-request overhead, enabling massive throughput improvements.

### Remote Client Benchmarks

Benchmarks from remote client (network latency ~150ms RTT to ap-south-1):

| Test | Go Client | Python Client | TypeScript Client |
|------|-----------|---------------|-------------------|
| **Single message** | 6.3 msg/s | - | - |
| **Batch 100 Ã— 1KB** | 112 msg/s | 331 msg/s | 284 msg/s |
| **8 Concurrent** | 152 msg/s | 567 msg/s | 571 msg/s |

> **Note:** Remote throughput is network-bound. Deploy producers close to GoQueue nodes for best performance.

### Performance Comparison

| System | Sequential | Batch (100) | Notes |
|--------|-----------|-------------|-------|
| **GoQueue** | ~320/s | ~30,000/s | Single binary, no dependencies |
| Kafka | ~100K/s | ~1M/s | Requires ZooKeeper/KRaft, JVM |
| RabbitMQ | ~10K/s | ~50K/s | Erlang-based |
| SQS | ~300/s | ~3,000/s | AWS managed, 10 msg batch limit |

For detailed benchmarks, see [docs/BENCHMARKS.md](docs/BENCHMARKS.md).

---

## Quick Start

### Using Docker (Local Development)

```bash
# Single node
docker run -p 8080:8080 -p 9000:9000 \
  -v goqueue-data:/var/lib/goqueue \
  ghcr.io/abd-ulbasit/goqueue:latest

# Verify it's running
curl http://localhost:8080/health
```

### Deploy to Kubernetes (AWS EKS)

```bash
# One-command deployment (creates EKS cluster + GoQueue)
cd deploy
./deploy.sh deploy dev

# Or deploy to existing cluster
./deploy.sh goqueue dev

# Get the LoadBalancer URL
./deploy.sh url dev

# Run benchmarks
./deploy.sh benchmark dev
```

### Using Helm (Custom Cluster)

```bash
# Add GoQueue to your cluster
helm install goqueue ./deploy/kubernetes/helm/goqueue \
  --set replicaCount=3 \
  --set service.type=LoadBalancer
```

### Using Go

```bash
go install github.com/abd-ulbasit/goqueue/cmd/goqueue@latest
goqueue -config config.yaml
```

### Produce Messages

```bash
# HTTP API
curl -X POST http://localhost:8080/v1/topics/orders/publish \
  -H "Content-Type: application/json" \
  -d '{
    "key": "order-123",
    "value": "eyJvcmRlcl9pZCI6MTIzfQ==",
    "delay": "5m"
  }'

# CLI
echo '{"order_id": 123}' | goqueue-cli produce orders --key order-123

# Go client
client.Publish(ctx, "orders", &goqueue.Message{
    Key:   []byte("order-123"),
    Value: orderJSON,
    Delay: 5 * time.Minute,
})
```

### Consume Messages

```bash
# CLI
goqueue-cli consume orders --group processors

# Go client
consumer := client.NewConsumer("processors", []string{"orders"})
for msg := range consumer.Messages() {
    process(msg)
    msg.Ack()  // Per-message acknowledgment
}
```

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              GoQueue Cluster                                 â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                         Coordination Layer                             â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚  â”‚
â”‚  â”‚  â”‚   Metadata   â”‚  â”‚   Leader     â”‚  â”‚  Consumer    â”‚                  â”‚  â”‚
â”‚  â”‚  â”‚    Store     â”‚  â”‚  Election    â”‚  â”‚  Coordinator â”‚                  â”‚  â”‚
â”‚  â”‚  â”‚  (embedded)  â”‚  â”‚  (Raft-lite) â”‚  â”‚ (rebalancing)â”‚                  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                          Storage Layer                                 â”‚  â”‚
â”‚  â”‚                                                                        â”‚  â”‚
â”‚  â”‚  Topic: orders                                                         â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚  â”‚
â”‚  â”‚  â”‚  Partition 0    â”‚  â”‚  Partition 1    â”‚  â”‚  Partition 2    â”‚         â”‚  â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ Log       â”‚  â”‚  â”‚  â”‚ Log       â”‚  â”‚  â”‚  â”‚ Log       â”‚  â”‚         â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ Segments  â”‚  â”‚  â”‚  â”‚ Segments  â”‚  â”‚  â”‚  â”‚ Segments  â”‚  â”‚         â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ Offset    â”‚  â”‚  â”‚  â”‚ Offset    â”‚  â”‚  â”‚  â”‚ Offset    â”‚  â”‚         â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ Index     â”‚  â”‚  â”‚  â”‚ Index     â”‚  â”‚  â”‚  â”‚ Index     â”‚  â”‚         â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ Time      â”‚  â”‚  â”‚  â”‚ Time      â”‚  â”‚  â”‚  â”‚ Time      â”‚  â”‚         â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ Index     â”‚  â”‚  â”‚  â”‚ Index     â”‚  â”‚  â”‚  â”‚ Index     â”‚  â”‚         â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ Delay     â”‚  â”‚  â”‚  â”‚ Delay     â”‚  â”‚  â”‚  â”‚ Delay     â”‚  â”‚         â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ Index     â”‚  â”‚  â”‚  â”‚ Index     â”‚  â”‚  â”‚  â”‚ Index     â”‚  â”‚         â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                        Delivery Layer                                  â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚  â”‚
â”‚  â”‚  â”‚ Timer Wheel  â”‚  â”‚  Visibility  â”‚  â”‚   Priority   â”‚                  â”‚  â”‚
â”‚  â”‚  â”‚ (delays)     â”‚  â”‚   Tracker    â”‚  â”‚    Router    â”‚                  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚  â”‚
â”‚  â”‚                                                                        â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚  â”‚
â”‚  â”‚  â”‚  DLQ Router  â”‚  â”‚   Tracing    â”‚  â”‚   Metrics    â”‚                  â”‚  â”‚
â”‚  â”‚  â”‚              â”‚  â”‚   Recorder   â”‚  â”‚   Collector  â”‚                  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Configuration

```yaml
broker:
  nodeId: "node-1"
  dataDir: "/var/lib/goqueue"
  
  listeners:
    http: ":8080"
    grpc: ":9000"
    
defaults:
  topic:
    partitions: 6
    replicationFactor: 2
    retention:
      hours: 168          # 7 days
      bytes: 10737418240  # 10GB
      
  delivery:
    visibilityTimeout: 30s
    maxRetries: 3
    dlqEnabled: true
    
  delay:
    enabled: true
    maxDelay: 168h        # 7 days max delay
    
  priority:
    lanes: 3              # high, normal, low
```

See [config.example.yaml](config.example.yaml) for full reference.

---

## Roadmap

See [ROADMAP.md](ROADMAP.md) for detailed milestone breakdown.

| Phase | Milestones | Status |
|-------|------------|--------|
| **Phase 1: Foundations** | Storage, Topics, Consumers, Reliability | ğŸ”œ Not Started |
| **Phase 2: Advanced Features** | Delays, Priority, Tracing, Schema | ğŸ“‹ Planned |
| **Phase 3: Distribution** | Clustering, Replication, Rebalancing | ğŸ“‹ Planned |
| **Phase 4: Operations** | APIs, CLI, Metrics, Kubernetes | ğŸ“‹ Planned |

---

## Why Build This?

This project exists to deeply understand distributed systems by building one. But it's not just a learning exerciseâ€”it solves real frustrations:

1. **Kafka's operational pain** inspired zero-dependency clustering and cooperative rebalancing
2. **SQS's limitations** inspired the hybrid ACK model with replay capability  
3. **Missing delay support everywhere** inspired native timer wheel integration
4. **Debugging nightmares** inspired built-in message tracing

Every feature choice came from real pain points, not feature checklists.

---

## License

MIT License - see [LICENSE](LICENSE) for details.

---

<p align="center">
  <em>Building queues to understand queues.</em>
</p>
