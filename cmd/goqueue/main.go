// =============================================================================
// GOQUEUE MAIN ENTRY POINT
// =============================================================================
//
// This is the entry point for the goqueue broker. It demonstrates:
//   - Creating a broker with default configuration
//   - Creating multi-partition topics
//   - Using the Producer with client-side batching
//   - Message routing via consistent hashing (Murmur3)
//   - HTTP API server for external access
//   - Consuming messages from partitions
//   - Graceful shutdown
//
// MILESTONE 2 FEATURES DEMONSTRATED:
//   - Multi-partition topics (default 3 partitions)
//   - Producer batching (100 msgs, 5ms linger, 64KB)
//   - Murmur3 hash partitioning for message key routing
//   - HTTP REST API (create topics, publish, consume)
//
// =============================================================================

package main

import (
	"context"
	"fmt"
	"log"
	"net/http"
	"os"
	"os/signal"
	"strings"
	"syscall"
	"time"

	"goqueue/internal/api"
	"goqueue/internal/broker"
	"goqueue/internal/grpc"
	"goqueue/internal/metrics"
)

func main() {
	fmt.Println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
	fmt.Println("â•‘                     GoQueue v0.2.0                            â•‘")
	fmt.Println("â•‘          Milestone 2: Topics, Partitions & Producer           â•‘")
	fmt.Println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
	fmt.Println()

	// -------------------------------------------------------------------------
	// STEP 1: Create broker with default configuration
	// -------------------------------------------------------------------------
	// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
	// â”‚ The Broker is the central component that:                               â”‚
	// â”‚   - Manages all topics and their partitions                             â”‚
	// â”‚   - Handles message persistence via the storage engine (M1)             â”‚
	// â”‚   - Coordinates producers and consumers                                 â”‚
	// â”‚                                                                         â”‚
	// â”‚ COMPARISON:                                                             â”‚
	// â”‚   - Kafka: Broker is a JVM process, typically 3+ in a cluster           â”‚
	// â”‚   - RabbitMQ: Broker = the entire RabbitMQ node                         â”‚
	// â”‚   - goqueue: Cluster mode enabled via GOQUEUE_CLUSTER_ENABLED           â”‚
	// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
	fmt.Println("ğŸ“¦ Starting broker...")
	config := broker.DefaultBrokerConfig()

	// Read data directory from environment variable (for Kubernetes deployments)
	// Falls back to ./data for local development
	if dataDir := os.Getenv("GOQUEUE_BROKER_DATADIR"); dataDir != "" {
		config.DataDir = dataDir
	} else {
		config.DataDir = "./data" // Default for local development
	}

	// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
	// â”‚ CLUSTER MODE CONFIGURATION                                              â”‚
	// â”‚                                                                         â”‚
	// â”‚ When GOQUEUE_CLUSTER_ENABLED=true, the broker runs in cluster mode:     â”‚
	// â”‚   - Joins other brokers via GOQUEUE_CLUSTER_PEERS                       â”‚
	// â”‚   - Participates in leader election                                     â”‚
	// â”‚   - Replicates data across nodes                                        â”‚
	// â”‚   - Synchronizes metadata (topics, partitions)                          â”‚
	// â”‚                                                                         â”‚
	// â”‚ KUBERNETES DEPLOYMENT:                                                  â”‚
	// â”‚   Each pod gets a stable DNS name via headless service:                 â”‚
	// â”‚     goqueue-0.goqueue-headless.namespace.svc.cluster.local:7000         â”‚
	// â”‚     goqueue-1.goqueue-headless.namespace.svc.cluster.local:7000         â”‚
	// â”‚     goqueue-2.goqueue-headless.namespace.svc.cluster.local:7000         â”‚
	// â”‚                                                                         â”‚
	// â”‚ COMPARISON:                                                             â”‚
	// â”‚   - Kafka: ZooKeeper (old) or KRaft (new) for coordination              â”‚
	// â”‚   - RabbitMQ: Erlang distribution for clustering                        â”‚
	// â”‚   - goqueue: Gossip-based membership + controller election              â”‚
	// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
	if os.Getenv("GOQUEUE_CLUSTER_ENABLED") == "true" {
		fmt.Println("ğŸ”— Cluster mode enabled")
		config.ClusterEnabled = true

		// ClientAdvertiseAddress is used for inter-node forwarding (where producers
		// should connect when forwarded). Falls back to listener address if not set.
		clientAdvertise := os.Getenv("GOQUEUE_CLUSTER_CLIENT_ADVERTISE")
		if clientAdvertise == "" {
			clientAdvertise = normalizeAddr(getEnvOrDefault("GOQUEUE_LISTENERS_HTTP", "8080"))
		}

		config.ClusterConfig = &broker.ClusterModeConfig{
			// normalizeAddr ensures we have a valid :port format
			// Handles both "8080" and ":8080" input formats
			ClientAddress:    clientAdvertise, // Use advertised address for forwarding
			ClusterAddress:   normalizeAddr(getEnvOrDefault("GOQUEUE_LISTENERS_INTERNAL", "7000")),
			AdvertiseAddress: os.Getenv("GOQUEUE_CLUSTER_ADVERTISE"),
			Peers:            splitPeers(os.Getenv("GOQUEUE_CLUSTER_PEERS")),
			QuorumSize:       getEnvIntOrDefault("GOQUEUE_CLUSTER_QUORUM", 2),
		}
		fmt.Printf("   âœ“ Peers: %v\n", config.ClusterConfig.Peers)
		fmt.Printf("   âœ“ Advertise: %s\n", config.ClusterConfig.AdvertiseAddress)
		fmt.Printf("   âœ“ Client Advertise: %s\n", config.ClusterConfig.ClientAddress)
	}

	// Read node ID from environment (for Kubernetes, this is the pod name)
	if nodeID := os.Getenv("GOQUEUE_BROKER_NODEID"); nodeID != "" {
		config.NodeID = nodeID
	}

	b, err := broker.NewBroker(config)
	if err != nil {
		log.Fatalf("Failed to create broker: %v", err)
	}
	defer b.Close()

	fmt.Printf("   âœ“ Broker started (NodeID: %s)\n", b.NodeID())
	fmt.Printf("   âœ“ Data directory: %s\n\n", b.DataDir())

	// -------------------------------------------------------------------------
	// STEP 1b: Initialize Prometheus Metrics (M17)
	// -------------------------------------------------------------------------
	// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
	// â”‚ PROMETHEUS METRICS - Observability for Production                       â”‚
	// â”‚                                                                         â”‚
	// â”‚ WHY METRICS MATTER:                                                     â”‚
	// â”‚   - Monitor message throughput (messages/sec)                           â”‚
	// â”‚   - Track latencies (p50, p95, p99)                                     â”‚
	// â”‚   - Alert on errors and anomalies                                       â”‚
	// â”‚   - Debug performance issues                                            â”‚
	// â”‚   - Capacity planning                                                   â”‚
	// â”‚                                                                         â”‚
	// â”‚ EXPOSED AT: http://localhost:8080/metrics                               â”‚
	// â”‚                                                                         â”‚
	// â”‚ METRICS CATEGORIES:                                                     â”‚
	// â”‚   - Broker: messages published/consumed, latencies, errors              â”‚
	// â”‚   - Storage: bytes written/read, fsync latency                          â”‚
	// â”‚   - Consumer: group members, lag, rebalances                            â”‚
	// â”‚   - Cluster: node health, leader elections, ISR changes                 â”‚
	// â”‚   - Go runtime: goroutines, memory, GC (optional)                       â”‚
	// â”‚                                                                         â”‚
	// â”‚ COMPARISON:                                                             â”‚
	// â”‚   - Kafka: JMX metrics, Confluent metrics reporter                      â”‚
	// â”‚   - RabbitMQ: Prometheus plugin                                         â”‚
	// â”‚   - SQS: CloudWatch metrics (AWS managed)                               â”‚
	// â”‚   - goqueue: Prometheus client_golang                                   â”‚
	// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
	fmt.Println("ğŸ“Š Initializing Prometheus metrics...")
	metricsConfig := metrics.DefaultConfig()
	metricsConfig.Enabled = true
	metricsConfig.IncludeGoCollector = true      // Include Go runtime metrics
	metricsConfig.IncludeProcessCollector = true // Include process metrics
	metrics.Init(metricsConfig)
	fmt.Println("   âœ“ Metrics initialized (endpoint: /metrics)")
	fmt.Println()

	// -------------------------------------------------------------------------
	// STEP 2: Create a multi-partition topic
	// -------------------------------------------------------------------------
	// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
	// â”‚ PARTITIONS - The Unit of Parallelism                                    â”‚
	// â”‚                                                                         â”‚
	// â”‚ A topic is split into partitions for:                                   â”‚
	// â”‚   1. PARALLELISM: Multiple consumers can read different partitions      â”‚
	// â”‚   2. ORDERING: Messages with same key go to same partition (ordered)    â”‚
	// â”‚   3. SCALABILITY: Partitions can be spread across nodes (future)        â”‚
	// â”‚                                                                         â”‚
	// â”‚ HOW IT WORKS:                                                           â”‚
	// â”‚   Producer â”€â”€â–º Topic â”€â”€â”¬â”€â”€ Partition 0 â”€â”€â–º Messages: A, D, G            â”‚
	// â”‚                        â”œâ”€â”€ Partition 1 â”€â”€â–º Messages: B, E, H            â”‚
	// â”‚                        â””â”€â”€ Partition 2 â”€â”€â–º Messages: C, F, I            â”‚
	// â”‚                                                                         â”‚
	// â”‚ ROUTING DECISION:                                                       â”‚
	// â”‚   - With key: hash(key) % numPartitions â†’ deterministic partition       â”‚
	// â”‚   - Without key: round-robin across partitions                          â”‚
	// â”‚                                                                         â”‚
	// â”‚ COMPARISON:                                                             â”‚
	// â”‚   - Kafka: Same model, partitions are fundamental unit                  â”‚
	// â”‚   - RabbitMQ: Queues (not partitions), different semantics              â”‚
	// â”‚   - SQS: FIFO queues have MessageGroupId (similar concept)              â”‚
	// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
	topicName := "demo-orders"
	numPartitions := 3

	if !b.TopicExists(topicName) {
		fmt.Printf("ğŸ“ Creating topic '%s' with %d partitions...\n", topicName, numPartitions)
		err := b.CreateTopic(broker.TopicConfig{
			Name:          topicName,
			NumPartitions: numPartitions,
		})
		if err != nil {
			log.Fatalf("Failed to create topic: %v", err)
		}
		fmt.Printf("   âœ“ Topic created\n\n")
	} else {
		fmt.Printf("ğŸ“‚ Topic '%s' already exists\n\n", topicName)
	}

	// -------------------------------------------------------------------------
	// STEP 3: Create a Producer with batching
	// -------------------------------------------------------------------------
	// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
	// â”‚ PRODUCER BATCHING - Trading Latency for Throughput                      â”‚
	// â”‚                                                                         â”‚
	// â”‚ Instead of sending each message immediately:                            â”‚
	// â”‚   1. Messages accumulate in an in-memory buffer                         â”‚
	// â”‚   2. Batch is flushed when ANY trigger fires:                           â”‚
	// â”‚      - BatchSize reached (100 messages)                                 â”‚
	// â”‚      - LingerMs elapsed (5ms since first message)                       â”‚
	// â”‚      - BatchBytes exceeded (64KB total)                                 â”‚
	// â”‚                                                                         â”‚
	// â”‚ FLOW:                                                                   â”‚
	// â”‚   Send() â”€â”€â–º Batch Buffer â”€â”€[trigger]â”€â”€â–º Flush to Broker               â”‚
	// â”‚                 â”‚                                                       â”‚
	// â”‚                 â”œâ”€â”€ size >= 100?    â”€â”€â–º flush                           â”‚
	// â”‚                 â”œâ”€â”€ age >= 5ms?     â”€â”€â–º flush                           â”‚
	// â”‚                 â””â”€â”€ bytes >= 64KB?  â”€â”€â–º flush                           â”‚
	// â”‚                                                                         â”‚
	// â”‚ COMPARISON:                                                             â”‚
	// â”‚   - Kafka: Same model (batch.size, linger.ms)                           â”‚
	// â”‚   - RabbitMQ: Publisher confirms, no client batching                    â”‚
	// â”‚   - SQS: SendMessageBatch (max 10 messages)                             â”‚
	// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
	fmt.Println("ğŸš€ Creating Producer with batching enabled...")
	producerConfig := broker.ProducerConfig{
		Topic:      topicName,
		BatchSize:  10,        // Smaller for demo (normally 100)
		LingerMs:   50,        // 50ms - longer for demo visibility
		BatchBytes: 64 * 1024, // 64KB
		AckMode:    broker.AckLeader,
	}

	producer, err := broker.NewProducer(b, producerConfig)
	if err != nil {
		log.Fatalf("Failed to create producer: %v", err)
	}
	defer producer.Close()

	fmt.Printf("   âœ“ Producer started (BatchSize=%d, LingerMs=%dms, AckMode=%s)\n\n",
		producerConfig.BatchSize, producerConfig.LingerMs, producerConfig.AckMode)

	// -------------------------------------------------------------------------
	// STEP 4: Publish messages with keys (demonstrates partitioning)
	// -------------------------------------------------------------------------
	// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
	// â”‚ KEY-BASED PARTITIONING - Ordering Guarantee                             â”‚
	// â”‚                                                                         â”‚
	// â”‚ When you provide a message key:                                         â”‚
	// â”‚   partition = murmur3(key) % numPartitions                              â”‚
	// â”‚                                                                         â”‚
	// â”‚ This ensures:                                                           â”‚
	// â”‚   - Same key ALWAYS goes to same partition                              â”‚
	// â”‚   - Messages for same key are ORDERED                                   â”‚
	// â”‚   - Different keys may share partitions (hash collisions)               â”‚
	// â”‚                                                                         â”‚
	// â”‚ EXAMPLE (3 partitions):                                                 â”‚
	// â”‚   "user-100" â†’ murmur3 â†’ partition 0                                    â”‚
	// â”‚   "user-200" â†’ murmur3 â†’ partition 2                                    â”‚
	// â”‚   "user-300" â†’ murmur3 â†’ partition 1                                    â”‚
	// â”‚   "user-100" â†’ murmur3 â†’ partition 0 (SAME!)                            â”‚
	// â”‚                                                                         â”‚
	// â”‚ WHY MURMUR3:                                                            â”‚
	// â”‚   - Fast (non-cryptographic)                                            â”‚
	// â”‚   - Excellent distribution (uniform across partitions)                  â”‚
	// â”‚   - Industry standard (Kafka default)                                   â”‚
	// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
	fmt.Println("ğŸ“¤ Publishing messages with keys (observing partition routing)...")
	messages := []struct {
		Key   string
		Value string
	}{
		// Orders from different users - should go to consistent partitions
		{"user-100", `{"order": "A", "user": "100", "product": "Widget"}`},
		{"user-200", `{"order": "B", "user": "200", "product": "Gadget"}`},
		{"user-300", `{"order": "C", "user": "300", "product": "Gizmo"}`},
		{"user-100", `{"order": "D", "user": "100", "product": "Sprocket"}`}, // Same user as A
		{"user-200", `{"order": "E", "user": "200", "product": "Cog"}`},      // Same user as B
		{"user-100", `{"order": "F", "user": "100", "product": "Bolt"}`},     // Same user as A, D
	}

	// Track which partition each user goes to
	userPartitions := make(map[string]int)

	for _, msg := range messages {
		// Use synchronous send for demo (easier to show partition assignment)
		ctx := context.Background()
		result := producer.SendSync(ctx, broker.ProducerRecord{
			Key:   []byte(msg.Key),
			Value: []byte(msg.Value),
		})
		if result.Error != nil {
			log.Printf("   âœ— Failed to publish: %v", result.Error)
			continue
		}

		partition := result.Partition
		offset := result.Offset

		// Track partition for this user
		if existing, ok := userPartitions[msg.Key]; ok {
			if existing != partition {
				fmt.Printf("   âš  PARTITION MISMATCH for %s! (expected %d, got %d)\n",
					msg.Key, existing, partition)
			}
		} else {
			userPartitions[msg.Key] = partition
		}

		fmt.Printf("   âœ“ key=%-10s â†’ partition=%d, offset=%d\n", msg.Key, partition, offset)
	}

	fmt.Println("\n   ğŸ“Š Partition Assignment Summary:")
	for user, part := range userPartitions {
		fmt.Printf("      %s â†’ Partition %d\n", user, part)
	}
	fmt.Println()

	// -------------------------------------------------------------------------
	// STEP 5: Consume messages from each partition
	// -------------------------------------------------------------------------
	fmt.Println("ğŸ“¥ Consuming messages from each partition...")
	for p := 0; p < numPartitions; p++ {
		consumed, err := b.Consume(topicName, p, 0, 100)
		if err != nil {
			log.Printf("   âœ— Failed to consume from partition %d: %v", p, err)
			continue
		}

		fmt.Printf("\n   Partition %d (%d messages):\n", p, len(consumed))
		for _, m := range consumed {
			key := string(m.Key)
			if key == "" {
				key = "(no key)"
			}
			fmt.Printf("      [offset=%d] key=%s\n", m.Offset, key)
		}
	}
	fmt.Println()

	// -------------------------------------------------------------------------
	// STEP 6: Start HTTP API Server
	// -------------------------------------------------------------------------
	// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
	// â”‚ HTTP API - External Access to GoQueue                                   â”‚
	// â”‚                                                                         â”‚
	// â”‚ Endpoints:                                                              â”‚
	// â”‚   GET  /health                              - Health check              â”‚
	// â”‚   GET  /stats                               - Broker statistics         â”‚
	// â”‚   POST /topics                              - Create topic              â”‚
	// â”‚   GET  /topics                              - List topics               â”‚
	// â”‚   GET  /topics/{name}                       - Get topic info            â”‚
	// â”‚   DELETE /topics/{name}                     - Delete topic              â”‚
	// â”‚   POST /topics/{name}/messages              - Publish messages          â”‚
	// â”‚   GET  /topics/{name}/partitions/{id}/msgs  - Consume messages          â”‚
	// â”‚                                                                         â”‚
	// â”‚ COMPARISON:                                                             â”‚
	// â”‚   - Kafka: Binary protocol (librdkafka), REST proxy separate            â”‚
	// â”‚   - RabbitMQ: AMQP protocol, HTTP management API                        â”‚
	// â”‚   - SQS: HTTP/REST API                                                  â”‚
	// â”‚   - goqueue: REST-first (gRPC planned for M14)                          â”‚
	// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
	fmt.Println("ğŸŒ Starting HTTP API server...")
	serverConfig := api.DefaultServerConfig()

	// Read HTTP listener address from environment variable (for Kubernetes)
	if httpAddr := os.Getenv("GOQUEUE_LISTENERS_HTTP"); httpAddr != "" {
		serverConfig.Addr = httpAddr
	} else {
		serverConfig.Addr = "127.0.0.1:8080" // Default for local development
	}

	server := api.NewServer(b, serverConfig)
	if err := server.Start(); err != nil {
		log.Fatalf("Failed to start HTTP server: %v", err)
	}

	fmt.Printf("   âœ“ HTTP API listening on http://%s\n", serverConfig.Addr)
	fmt.Println()

	// -------------------------------------------------------------------------
	// STEP 6b: Start gRPC Server (M15)
	// -------------------------------------------------------------------------
	// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
	// â”‚ gRPC API - High-Performance Binary Protocol                             â”‚
	// â”‚                                                                         â”‚
	// â”‚ WHY gRPC alongside HTTP:                                                â”‚
	// â”‚   - HTTP: Easy debugging, curl-friendly, wide compatibility             â”‚
	// â”‚   - gRPC: High performance, streaming, type-safe (for hot path)         â”‚
	// â”‚                                                                         â”‚
	// â”‚ gRPC Services:                                                          â”‚
	// â”‚   PublishService  - Message publishing (unary + streaming)              â”‚
	// â”‚   ConsumeService  - Message consuming (streaming)                       â”‚
	// â”‚   AckService      - Message acknowledgment                              â”‚
	// â”‚   OffsetService   - Consumer offset management                          â”‚
	// â”‚   HealthService   - Health checking (gRPC standard)                     â”‚
	// â”‚                                                                         â”‚
	// â”‚ COMPARISON:                                                             â”‚
	// â”‚   - Kafka: Custom binary protocol over TCP                              â”‚
	// â”‚   - RabbitMQ: AMQP protocol (binary)                                    â”‚
	// â”‚   - NATS: Custom binary protocol                                        â”‚
	// â”‚   - goqueue: gRPC/HTTP2 with Protocol Buffers                           â”‚
	// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
	fmt.Println("ğŸ”Œ Starting gRPC server...")

	grpcConfig := grpc.DefaultServerConfig()

	// Read gRPC listener address from environment variable (for Kubernetes)
	if grpcAddr := os.Getenv("GOQUEUE_LISTENERS_GRPC"); grpcAddr != "" {
		grpcConfig.Address = grpcAddr
	} else {
		grpcConfig.Address = "127.0.0.1:9000" // Default for local development
	}
	grpcConfig.EnableReflection = true // Enable for debugging with grpcurl

	grpcServer := grpc.NewServer(b, grpcConfig)

	// Start gRPC server in a goroutine (it blocks until Stop() is called)
	go func() {
		if err := grpcServer.Start(); err != nil {
			log.Fatalf("Failed to start gRPC server: %v", err)
		}
	}()

	// Small delay to ensure gRPC server is listening before marking as ready
	time.Sleep(100 * time.Millisecond)

	fmt.Printf("   âœ“ gRPC API listening on %s\n", grpcConfig.Address)
	fmt.Println()

	// -------------------------------------------------------------------------
	// STEP 6c: Start Cluster HTTP Server (if cluster mode enabled)
	// -------------------------------------------------------------------------
	// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
	// â”‚ CLUSTER INTER-NODE COMMUNICATION                                        â”‚
	// â”‚                                                                         â”‚
	// â”‚ WHY SEPARATE HTTP SERVER:                                               â”‚
	// â”‚   - Cluster traffic is internal (between nodes), not client-facing      â”‚
	// â”‚   - Separate port (7000) allows network policies to isolate traffic     â”‚
	// â”‚   - Uses Go 1.22 http.ServeMux (simpler than chi for internal API)      â”‚
	// â”‚   - Must be running BEFORE coordinator tries to join cluster            â”‚
	// â”‚                                                                         â”‚
	// â”‚ ENDPOINTS (port 7000):                                                  â”‚
	// â”‚   POST /cluster/heartbeat  - Health check between nodes                 â”‚
	// â”‚   POST /cluster/join       - Node requesting to join cluster            â”‚
	// â”‚   POST /cluster/leave      - Node gracefully departing                  â”‚
	// â”‚   GET  /cluster/state      - Get cluster membership state               â”‚
	// â”‚   POST /cluster/vote       - Controller election vote request           â”‚
	// â”‚   POST /cluster/metadata   - Sync topic metadata from controller        â”‚
	// â”‚   GET  /cluster/health     - Cluster health status                      â”‚
	// â”‚                                                                         â”‚
	// â”‚ STARTUP ORDER (CRITICAL):                                               â”‚
	// â”‚   1. Main HTTP server starts (port 8080) âœ“                              â”‚
	// â”‚   2. gRPC server starts (port 9000) âœ“                                   â”‚
	// â”‚   3. Cluster HTTP server starts (port 7000) <- WE ARE HERE              â”‚
	// â”‚   4. Coordinator bootstrap (joins/forms cluster)                        â”‚
	// â”‚   5. Mark as ready                                                      â”‚
	// â”‚                                                                         â”‚
	// â”‚ This order ensures peers can receive join requests before anyone tries  â”‚
	// â”‚ to join, preventing the deadlock where all pods fail to connect.        â”‚
	// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
	if b.IsClusterEnabled() {
		// Get cluster address from config (default :7000)
		clusterAddr := getEnvOrDefault("GOQUEUE_LISTENERS_INTERNAL", ":7000")
		clusterAddr = normalizeAddr(clusterAddr)

		fmt.Printf("ğŸ”— Starting cluster HTTP server on %s...\n", clusterAddr)

		// Create a separate http.ServeMux for cluster endpoints
		clusterMux := http.NewServeMux()
		b.RegisterClusterRoutes(clusterMux)

		// Create and start the cluster HTTP server
		clusterServer := &http.Server{
			Addr:         clusterAddr,
			Handler:      clusterMux,
			ReadTimeout:  30 * time.Second,
			WriteTimeout: 30 * time.Second,
			IdleTimeout:  60 * time.Second,
		}

		// Start cluster server in a goroutine
		go func() {
			if err := clusterServer.ListenAndServe(); err != nil && err != http.ErrServerClosed {
				log.Printf("Cluster HTTP server error: %v", err)
			}
		}()

		// Give the listener time to start
		time.Sleep(100 * time.Millisecond)
		fmt.Printf("   âœ“ Cluster HTTP server listening on %s\n", clusterAddr)

		// Now that HTTP is listening, start the cluster coordinator
		fmt.Println("   âœ“ Starting cluster coordinator...")
		if err := b.StartCluster(); err != nil {
			log.Fatalf("Failed to start cluster: %v", err)
		}
		fmt.Println("   âœ“ Cluster coordinator started")
		fmt.Println()
	}

	// -------------------------------------------------------------------------
	// Mark server as ready for Kubernetes readiness probe
	// -------------------------------------------------------------------------
	api.GetHealthState().SetReady(true)
	fmt.Println("   âœ“ Server marked as ready")
	fmt.Println()

	fmt.Println("   Try these commands:")
	fmt.Println("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
	fmt.Println("   â”‚ HTTP API (debugging):                                                  â”‚")
	fmt.Println("   â”‚   curl http://localhost:8080/health                                    â”‚")
	fmt.Println("   â”‚   curl http://localhost:8080/stats                                     â”‚")
	fmt.Println("   â”‚   curl http://localhost:8080/topics                                    â”‚")
	fmt.Println("   â”‚   curl -X POST -d '{\"name\":\"test\"}' http://localhost:8080/topics    â”‚")
	fmt.Println("   â”‚                                                                        â”‚")
	fmt.Println("   â”‚ gRPC API (high performance):                                           â”‚")
	fmt.Println("   â”‚   Use the goqueue Go client for gRPC operations                        â”‚")
	fmt.Println("   â”‚   grpcurl -plaintext localhost:9000 list                               â”‚")
	fmt.Println("   â”‚   grpcurl -plaintext localhost:9000 goqueue.v1.HealthService/Check     â”‚")
	fmt.Println("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
	fmt.Println()

	// -------------------------------------------------------------------------
	// STEP 7: Show statistics
	// -------------------------------------------------------------------------
	fmt.Println("ğŸ“Š Broker Statistics:")
	stats := b.Stats()
	fmt.Printf("   Node ID:     %s\n", stats.NodeID)
	fmt.Printf("   Uptime:      %s\n", stats.Uptime.Round(time.Millisecond))
	fmt.Printf("   Topics:      %d\n", stats.TopicCount)
	fmt.Printf("   Total Size:  %d bytes\n", stats.TotalSize)
	for name, ts := range stats.TopicStats {
		fmt.Printf("\n   Topic '%s':\n", name)
		fmt.Printf("     Partitions: %d\n", ts.Partitions)
		fmt.Printf("     Messages:   %d\n", ts.TotalMessages)
		fmt.Printf("     Size:       %d bytes\n", ts.TotalSize)
	}

	// -------------------------------------------------------------------------
	// STEP 8: Wait for interrupt (server mode)
	// -------------------------------------------------------------------------
	fmt.Println()
	fmt.Println("ğŸš€ GoQueue running. Press Ctrl+C to stop.")

	// Set up signal handling for graceful shutdown
	sigCh := make(chan os.Signal, 1)
	signal.Notify(sigCh, syscall.SIGINT, syscall.SIGTERM)
	<-sigCh

	fmt.Println("\n\nğŸ›‘ Shutting down...")

	// Graceful shutdown with timeout
	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer cancel()

	// Stop gRPC server first (handles in-flight RPCs)
	grpcServer.Stop()
	fmt.Println("   âœ“ gRPC server stopped")

	// Stop HTTP server
	if err := server.Stop(ctx); err != nil {
		log.Printf("HTTP server shutdown error: %v", err)
	}
	fmt.Println("   âœ“ HTTP server stopped")

	fmt.Println("   âœ“ Shutdown complete")
}

// =============================================================================
// HELPER FUNCTIONS FOR ENVIRONMENT CONFIGURATION
// =============================================================================

// getEnvOrDefault returns the environment variable value or a default.
func getEnvOrDefault(key, defaultValue string) string {
	if v := os.Getenv(key); v != "" {
		return v
	}
	return defaultValue
}

// getEnvIntOrDefault returns the environment variable as int or a default.
func getEnvIntOrDefault(key string, defaultValue int) int {
	if v := os.Getenv(key); v != "" {
		if i, err := parseIntSafe(v); err == nil {
			return i
		}
	}
	return defaultValue
}

// parseIntSafe parses a string to int safely.
func parseIntSafe(s string) (int, error) {
	var i int
	_, err := fmt.Sscanf(s, "%d", &i)
	return i, err
}

// splitPeers splits a comma-separated list of peers into a slice.
// Empty string returns empty slice (single-node mode).
func splitPeers(peers string) []string {
	if peers == "" {
		return nil
	}
	var result []string
	for _, p := range splitString(peers, ",") {
		p = trimSpace(p)
		if p != "" {
			result = append(result, p)
		}
	}
	return result
}

// splitString splits a string by separator (simple implementation).
func splitString(s, sep string) []string {
	if s == "" {
		return nil
	}
	var result []string
	start := 0
	for i := 0; i <= len(s)-len(sep); i++ {
		if s[i:i+len(sep)] == sep {
			result = append(result, s[start:i])
			start = i + len(sep)
			i += len(sep) - 1
		}
	}
	result = append(result, s[start:])
	return result
}

// trimSpace removes leading and trailing whitespace.
func trimSpace(s string) string {
	start := 0
	end := len(s)
	for start < end && (s[start] == ' ' || s[start] == '\t' || s[start] == '\n' || s[start] == '\r') {
		start++
	}
	for end > start && (s[end-1] == ' ' || s[end-1] == '\t' || s[end-1] == '\n' || s[end-1] == '\r') {
		end--
	}
	return s[start:end]
}

// normalizeAddr ensures address is in :port format.
// Handles these input formats:
//   - "8080"         â†’ ":8080"
//   - ":8080"        â†’ ":8080"
//   - "0.0.0.0:8080" â†’ ":8080" (extracts port from full address)
//   - "host:8080"    â†’ ":8080" (extracts port from host:port)
func normalizeAddr(addr string) string {
	addr = trimSpace(addr)
	if addr == "" {
		return ""
	}
	// Check if it contains a colon (could be host:port or just :port)
	if colonIdx := strings.LastIndex(addr, ":"); colonIdx >= 0 {
		// Extract just the port part
		port := addr[colonIdx:]
		return port
	}
	// No colon, assume it's just a port number
	return ":" + addr
}
