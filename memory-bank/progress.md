# GoQueue - Progress

## Overall Status

**Phase**: 4 of 4
**Milestones**: 20/26 complete (M11: Leader Election & Replication - IN PROGRESS)
**Tests**: 640+ passing (storage: 50+, broker: 470+, api: 24, grpc: 16, client: 14, cluster: 50+)
**Started**: Session 1

## Latest Session - Topic Metadata Sync & Partition API

### What Was Accomplished

**Topic Metadata Sync (FIXED)**:
- **Problem**: Topics created on controller weren't visible on followers for replication
- **Root Cause**: `handleMetadataChange` in cluster_integration.go was a stub that only logged
- **Fix**: Added `ensureLocalTopic()` that creates topics locally when metadata is synced
- Added `CreateTopicLocal()` to broker - creates topic without re-registering with cluster metadata
- Now when controller creates a topic, followers automatically create it locally via metadata sync

**Partition Leader API (NEW)**:
- Added `GET /topics/{topicName}/partitions` endpoint
- Returns: partition number, leader, replicas, ISR, version for each partition
- Example response:
  ```json
  {
    "partitions": [
      {"partition": 0, "leader": "goqueue-0", "replicas": ["goqueue-0", "goqueue-1", "goqueue-2"], "isr": ["goqueue-0", "goqueue-1", "goqueue-2"], "version": 1}
    ],
    "topic": "sync-test3"
  }
  ```

**Failover Testing (VERIFIED)**:
- Tested node death by scaling down: `kubectl scale statefulset goqueue --replicas=2`
- Failure detector correctly identifies dead nodes (~12s)
- Leader election occurs when nodes rejoin
- ISR tracking works - all 3 nodes in ISR when healthy
- Epoch/version increments on leadership changes
- Unclean election prevention working: "no ISR available, unclean election disabled"

### Code Changes

**`internal/broker/cluster_integration.go`**:
- `handleMetadataChange()`: Now iterates through all topics in metadata and calls `ensureLocalTopic()`
- Added `ensureLocalTopic(name, partitions)`: Creates topic locally from cluster metadata

**`internal/broker/broker.go`**:
- Added `CreateTopicLocal(config TopicConfig)`: Creates topic without cluster metadata registration
- Used for metadata sync - idempotent (returns nil if topic exists)

**`internal/api/server.go`**:
- Added route: `r.Get("/partitions", s.getTopicPartitions)`
- Handler calls `b.GetTopicPartitions(topicName)` and returns JSON

### Test Results

- 3-node EKS cluster running with v0.5.0-isr3
- Topic creation on controller syncs to all followers
- Partition API returns correct leader/replica/ISR information
- Failover tested: node death detected, re-election works

### Docker Images

- `v0.5.0-isr3`: Latest with metadata sync fix
- Built with cross-compilation: `CGO_ENABLED=0 GOOS=linux GOARCH=amd64`

### Known Gaps

1. **Request Routing**: Writes to non-leader partitions go locally instead of forwarding to leader
2. **Synchronous Replication**: WaitForReplication is called but may not wait for ISR ack
3. **Leader Election on Death**: Works but needs more testing

---

## Previous Session - Cluster Formation Fix & Failover Detection

### What Was Accomplished

**CRITICAL BUG FIX: Coordinator Context Lifecycle**:
- **Root Cause**: Bootstrap used `context.WithTimeout(context.Background(), 60s)` 
- The coordinator's `c.ctx` was derived from this timeout context
- When bootstrap completed and `defer cancel()` ran, it cancelled the coordinator's context
- This caused `heartbeatLoop` to exit immediately (context cancelled)
- **Fix**: Use `context.Background()` for coordinator's long-running context

**Fixed Heartbeat Loop**:
- Previously: Heartbeat loop started then immediately stopped (context cancelled)
- Now: Heartbeat loop runs continuously, sending heartbeats every 3s
- Verified: Nodes stay alive without being marked dead

**Failure Detection Working**:
- Failure detector correctly identifies nodes that stop heartbeating
- Suspect timeout: 6s (2 missed heartbeats)
- Dead timeout: 9s (3 missed heartbeats)
- Tested: Killed pod, node was marked suspect‚Üídead correctly

**Controller Election on Death**:
- When controller dies, remaining nodes trigger new election
- Log observed: "controller died, triggering election"
- New controller elected successfully

**Partition Failover Code**:
- Added `ElectLeadersForNode()` call in `handleMembershipEvent`
- Triggers partition leader election when a node dies
- Only runs on controller node

### Code Changes

**`internal/cluster/coordinator.go`**:
- Fixed context lifecycle: `c.ctx, c.cancel = context.WithCancel(context.Background())`
- Added `partitionElector *PartitionLeaderElector` field
- `handleMembershipEvent` triggers partition failover on `EventNodeDied`

**`internal/cluster/cluster_server.go`**:
- `handleJoin` now calls `RecordHeartbeat` for joining nodes
- `BroadcastHeartbeats` uses `GetOtherNodes()` (includes suspect, skips dead)

**`internal/cluster/failure_detector.go`**:
- `Start()` seeds initial heartbeats for existing nodes
- Added membership listener for newly joined nodes
- Detection loop correctly marks nodes as suspect‚Üídead

**`internal/cluster/membership.go`**:
- Fixed `ApplyState` version comparison for Version=0 case

### Test Results

- 3-node EKS cluster running stable
- All nodes remain alive with continuous heartbeats
- Failure detection tested: killed node detected as dead within 9s
- Controller re-election verified
- Cluster recovers when killed pod restarts

### Docker Images Built

- `v0.5.0-failover` through `v0.5.0-failover9`
- Latest stable: `v0.5.0-failover9`

### Known Issues

1. **Topic Sync**: Topics created on one node not immediately visible on others
2. **Partition Leader API**: No endpoint to view partition leaders
3. **Kubernetes restart**: When pod restarts quickly, failover partially executes

---

## Previous Session - 3-Node Cluster Deployment & High-Throughput Benchmarks

### What Was Built

**Cluster Deployment Bug Fixes**:
- Fixed `normalizeAddr()` function in `cmd/goqueue/main.go` - was prepending `:` to full addresses like `0.0.0.0:8080` creating invalid `:0.0.0.0:8080`
- Added `strings` import for `LastIndex` usage
- Docker image rebuilt and pushed as `v0.4.1` and `latest`

**Docker Build Optimization**:
- Created `.dockerignore` reducing build context from 1GB to 65MB
- Excludes: .git/, data/, website/, docs/, terraform state, node_modules/

**StatefulSet Configuration** (`deploy/kubernetes/manual/statefulset.yaml`):
- `podManagementPolicy: Parallel` for simultaneous pod startup
- Environment variables for cluster mode:
  - `GOQUEUE_BROKER_NODEID` from pod name
  - `GOQUEUE_CLUSTER_ENABLED=true`
  - `GOQUEUE_CLUSTER_PEERS` with all 3 node addresses
  - `GOQUEUE_CLUSTER_ADVERTISE` for inter-node discovery
  - `GOQUEUE_LISTENERS_HTTP/GRPC/INTERNAL` bound to 0.0.0.0

**Benchmark Jobs**:
- Created `publish-benchmark.yaml` for comprehensive publish testing
- Tests: Sequential, concurrent (4-32 threads), batch (10-1000 msgs)

### Benchmark Results (3-Node EKS Cluster, c5.xlarge)

**In-Cluster Benchmarks** (minimal network latency):

| Mode | Configuration | Throughput |
|------|--------------|------------|
| Sequential | 1 msg at a time | **~320 msgs/sec** |
| Concurrent | 8 threads | **~1,300 msgs/sec** |
| Batch (100) | 100 msgs/batch | **~30,000 msgs/sec** |
| Batch (1000) | 1000 msgs/batch | **~220,000 msgs/sec** |

**Scaling Analysis**:
```
Batch Size    Throughput       vs Sequential
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
     1        ~320/s           baseline
    10        ~3,000/s         ~10x
   100        ~30,000/s        ~100x
   500        ~130,000/s       ~400x
  1000        ~220,000/s       ~700x
```

**Concurrent Thread Scaling**:
```
Threads    Throughput    vs Single Thread
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   4       ~1,140/s      3.5x
   8       ~1,300/s      4x (optimal)
  16       ~1,030/s      diminishing returns
  32       ~1,030/s      contention
```

### Documentation Updates

**Created `docs/BENCHMARKS.md`**:
- Full benchmark methodology and results
- Hardware recommendations by workload
- Comparison with other systems
- Instructions to run benchmarks

**Updated `README.md` Performance Section**:
- In-cluster benchmark results
- Batch scaling chart
- Comparison table with Kafka, RabbitMQ, SQS

### Key Learnings

**Network is the Bottleneck for Sequential Publish**:
- Remote (150ms RTT): ~30 msgs/sec
- In-cluster: ~320 msgs/sec (10x faster)
- Batch mode: eliminates network as bottleneck

**Batch Amortizes Per-Request Overhead**:
- TCP connection, HTTP parsing, logging all happen once per batch
- 100 msg batch = 100x throughput improvement
- 1000 msg batch = 700x throughput improvement

**Cluster State Not Yet Replicated**:
- Topics created on one node aren't visible to others
- LoadBalancer routes to different pods = inconsistent state
- This is expected - topic replication is future work

### Files Changed

- `cmd/goqueue/main.go` - Fixed normalizeAddr function
- `.dockerignore` - Created for build optimization
- `deploy/kubernetes/manual/statefulset.yaml` - Cluster env vars
- `deploy/kubernetes/manual/publish-benchmark.yaml` - Benchmark job
- `deploy/kubernetes/manual/full-benchmark.yaml` - Full test suite
- `docs/BENCHMARKS.md` - Performance documentation
- `README.md` - Updated Performance section

---

## Previous Session - EKS Benchmarks & Deployment Improvements

### What Was Built

**Benchmarking Infrastructure**:
- Fixed benchmark tests to use correct API field (`num_partitions` not `partitions`)
- Fixed Python benchmark API paths (removed `/api/v1/` prefix, fixed poll endpoint)
- Fixed TypeScript benchmark API paths (same fixes as Python)

**Benchmark Results (AWS EKS, ap-south-1, LoadBalancer, ~150ms RTT)**:

| Test | Go | Python | TypeScript |
|------|-----|--------|------------|
| Single message (1KB) | 6.3 msg/s | - | - |
| Batch 100 √ó 1KB | 112 msg/s | 331 msg/s | 284 msg/s |
| Batch 100 √ó 100B | 365 msg/s | - | - |
| Batch 1000 √ó 100B | 1,090 msg/s | - | - |
| 8 Concurrent | 152 msg/s | 567 msg/s | 571 msg/s |
| Sustained (30s) | - | 552 msg/s | 549 msg/s |

**Deployment Simplification** (`deploy/deploy.sh`):
- One-command EKS deployment: `./deploy.sh deploy dev`
- GoQueue-only deployment: `./deploy.sh goqueue dev`
- LoadBalancer URL retrieval: `./deploy.sh url dev`
- Status check: `./deploy.sh status dev`
- Benchmark runner: `./deploy.sh benchmark dev`
- Teardown: `./deploy.sh destroy dev`

**Docker Image Updates**:
- Pushed `ghcr.io/abd-ulbasit/goqueue:latest` with K8s fixes
- Tagged as `v0.2.0` for release tracking
- Kubernetes fixes: env vars for data directory and listener addresses

**README Updates**:
- Added Performance section with benchmark results
- Updated Quick Start with Kubernetes deployment instructions
- Added Helm installation command

### Key Learnings

**Multi-replica Load Balancing Issue**:
- With 3 replicas, topic created on pod1 but publish routed to pod2
- Root cause: pods have independent state (not clustered)
- Solution: Scale to 1 replica OR implement proper clustering

**Network Latency Impact**:
- Single-message throughput limited by RTT (~150ms each way)
- Batching is critical for throughput (100x improvement)
- Concurrent producers help saturate network

**Client Language Comparison**:
- Go client: Fastest raw throughput with small batches
- Python/TS: Better sustained throughput with HTTP connection pooling
- All clients: Similar performance with concurrent producers

---

## Previous Session - QuotaEnforcer Refactoring

### What Was Built

**QuotaEnforcer Strategy Pattern** (`internal/broker/quota_enforcer.go`):
- Interface for abstracting quota enforcement
- Methods: CheckPublish, CheckPublishBatch, CheckConsume, CheckTopicCreation, CheckConsumerGroup, CheckDelay, TrackUsage, IsEnabled
- Two implementations:
  - `NoOpEnforcer`: Single-tenant mode - all methods return nil (zero overhead)
  - `TenantQuotaEnforcer`: Multi-tenant mode - delegates to QuotaManager

**Why**: Eliminated 14 scattered `if b.tenantManager != nil` checks in tenant_broker.go

**Broker Changes** (`internal/broker/broker.go`):
- Added `quotaEnforcer QuotaEnforcer` field to Broker struct
- Initialization sets appropriate enforcer based on EnableMultiTenancy config

**tenant_broker.go Refactoring**:
- Replaced all 14 nil checks with direct `b.quotaEnforcer.CheckX()` calls
- Clean, testable code without conditionals

**CLI Consistency** (`cmd/goqueue-admin/`):
- Added `handleError()` function matching goqueue-cli pattern
- Updated 15 API error handlers across tenant.go, quota.go, usage.go
- Consistent user-facing error output using `cli.PrintError()`

### Pattern Learned: Strategy for Optional Features

When a feature is optional (like multi-tenancy), use strategy pattern:
1. Define interface with all operations
2. Create NoOp implementation (does nothing, zero cost)
3. Create real implementation (actual logic)
4. Select at initialization based on config

This gives: clean code, zero overhead when disabled, testability.

---

## Milestone 18 - Multi-Tenancy & Quotas ‚≠ê (COMPLETE!)

### What Was Built

**Optional Multi-Tenancy** (`internal/broker/broker.go`):
- `EnableMultiTenancy bool` config flag (default: false)
- TenantManager initialized only when enabled
- Single-tenant mode: zero overhead, direct topic access
- Multi-tenant mode: full isolation and quotas

**Deployment Models**:
- **Single-tenant (default)**: For K8s deployments where each customer gets own cluster
- **Multi-tenant**: For managed service / SaaS deployments with quotas

**Tenant Management** (`internal/broker/tenant.go`):
- Tenant entity with ID, name, status (active/suspended/disabled), quotas, metadata
- TenantManager for CRUD operations with file-based persistence
- Namespace isolation via topic prefix pattern: `{tenantID}.{topicName}`
- System tenant (`__system`) for internal topics
- Tenant lifecycle: create ‚Üí active ‚Üí suspended ‚Üí active ‚Üí disabled
- Usage tracking: messages, bytes, topics, partitions, connections

**Token Bucket Rate Limiting** (`internal/broker/quota.go`):
- Industry-standard token bucket algorithm (O(1), allows bursts)
- Per-tenant rate limiting for publish and consume
- Configurable capacity and refill rate
- Thread-safe implementation with atomic operations

**Quota Manager** (`internal/broker/quota_manager.go`):
- Centralized quota enforcement
- Check methods for all quota types:
  - `CheckPublishRate`, `CheckConsumeRate` (message rate)
  - `CheckPublishBytesRate`, `CheckConsumeBytesRate` (throughput)
  - `CheckMessageSize` (single message limit)
  - `CheckStorageQuota` (total storage per tenant)
  - `CheckTopicCreation` (topic count, partition count)
  - `CheckConsumerGroupCount` (connection limits)
  - `CheckDelay` (max delay for scheduled messages)
- Violation tracking per tenant

**Broker Integration** (`internal/broker/tenant_broker.go`):
- `IsMultiTenantEnabled()` method
- Tenant-aware broker methods:
  - `PublishForTenant`, `PublishBatchForTenant`
  - `ConsumeForTenant`
  - `CreateTopicForTenant`, `DeleteTopicForTenant`, `ListTopicsForTenant`
  - `PublishWithDelayForTenant`, `PublishWithPriorityForTenant`
  - `JoinGroupForTenant`
- Quota enforcement at broker layer (catches all paths, bypassed when disabled)
- Usage tracking after successful operations

**HTTP API** (`internal/api/tenant_api.go`):
- REST endpoints at `/admin/tenants/*`:
  - CRUD: `POST /admin/tenants`, `GET /admin/tenants/{id}`, etc.
  - Lifecycle: `POST /admin/tenants/{id}/suspend|activate|disable`
  - Quotas: `GET|PUT /admin/tenants/{id}/quotas`
  - Usage: `GET /admin/tenants/{id}/usage|stats`
  - Resources: `GET /admin/tenants/{id}/topics`
- Returns 503 when multi-tenancy disabled

**Admin CLI** (`cmd/goqueue-admin/`):
- Separate CLI for superadmin operations
- Uses shared `internal/cli` package (same as goqueue-cli)
- Commands:
  - `tenant create|list|get|delete|suspend|activate|disable`
  - `quota get|update|reset`
  - `usage get`
- Table/JSON/YAML output formats

### Architecture Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Default mode | Single-tenant | K8s deployments need no multi-tenant overhead |
| Namespace isolation | Topic prefix | Simple, works with existing storage, no major refactoring |
| Rate limiting | Token bucket | O(1), allows bursts, industry standard |
| Quota enforcement | Broker layer | Catches all paths, not just API |
| Persistence | File-based JSON | Simple, reliable, matches existing patterns |
| When disabled | Skip all checks | Zero overhead, direct topic access |
| Quota types priority | Rate ‚Üí Storage ‚Üí Count ‚Üí Size | Rate limits are most time-sensitive |
| Exceeded behavior | Reject immediately | Clear error, no partial processing |

### Key Concepts Learned

**Multi-tenancy approaches**:
- Namespace isolation (topic prefix) - goqueue, Kafka conventions
- Virtual hosts - RabbitMQ
- Account-level isolation - AWS SQS

**Token Bucket Algorithm**:
- Tokens refill at constant rate up to capacity
- Operations consume tokens
- Allows bursts up to capacity
- O(1) operations, thread-safe

**Quota types**:
- Rate quotas (msg/sec, bytes/sec) - controlled by token bucket
- Storage quotas (total bytes, topic count) - simple threshold checks
- Connection quotas (concurrent connections, consumer groups)

## Critical Issues Fixed (2026-01-11)

### Category 1 - All Fixed ‚úÖ
1. **M5+M6 Integration** - Added `PublishWithDelayAndPriority` and `PublishAtWithPriority` methods
2. **Delay Filtering** - Was already implemented (stale TODO removed)
3. **Transaction Abort Retry** - Added `abortTransactionWithRetry` with exponential backoff
4. **OTLP/Jaeger** - Replaced stubs with official OpenTelemetry SDK implementation

### Race Condition Fixes (2026-01-11)
1. **GetNode Data Race** - `ClusterState.GetNode()` now returns a clone to prevent races
2. **Event Listener Race** - `TestMembership_EventListener` now uses mutex for event tracking

### Category 2+3 - Milestone Created
- See [M15-performance-cluster-optimizations.md](tasks/M15-performance-cluster-optimizations.md)

## Phase Progress

### Phase 1: Foundations (4/4) ‚úÖ
- [x] Milestone 1: Storage Engine & Append-Only Log ‚úÖ
- [x] Milestone 2: Topics, Partitions & Producer API ‚úÖ
- [x] Milestone 3: Consumer Groups & Offset Management ‚úÖ
- [x] Milestone 4: Reliability - ACKs, Visibility & DLQ ‚úÖ

### Phase 2: Advanced Features (5/5) ‚úÖ
- [x] Milestone 5: Native Delay & Scheduled Messages ‚úÖ
- [x] Milestone 6: Priority Lanes ‚úÖ ‚≠ê
- [x] Milestone 7: Message Tracing ‚úÖ ‚≠ê
- [x] Milestone 8: Schema Registry ‚úÖ
- [x] Milestone 9: Transactional Publish ‚úÖ ‚≠ê

### Phase 3: Distribution (5/5) ‚úÖ
- [x] Milestone 10: Cluster Formation & Metadata ‚úÖ ‚≠ê
- [x] Milestone 11: Leader Election & Replication ‚úÖ 
- [x] Milestone 12: Cooperative Rebalancing ‚úÖ ‚≠ê
- [x] Milestone 13: Online Partition Scaling ‚úÖ ‚≠ê
- [x] Milestone 14: Log Compaction, Snapshots & Time Index ‚≠ê 

### Phase 4: Operations (3/12)
- [x] Milestone 15: gRPC API & Go Client ‚úÖ ‚≠ê
- [ ] Milestone 16: CLI Tool
- [ ] Milestone 17: Prometheus Metrics & Grafana
- [x] Milestone 18: Multi-Tenancy & Quotas ‚úÖ ‚≠ê
- [x] Milestone 19: Kubernetes & Chaos Testing ‚úÖ ‚≠ê
- [ ] Milestone 20: Final Review & Documentation with Examples and Comparison to Alternatives
- [ ] Milestone 21: Buffer Pooling & Performance Tuning
- [ ] Milestone 22: Security - TLS, Auth, RBAC
- [ ] Milestone 23: Backup & Restore
- [ ] Milestone 24: Monitoring & Alerting
- [ ] Milestone 25: Production Hardening & Best Practices
- [ ] Milestone 26: Release Process & CI/CD

---

## Milestone 19 - Kubernetes & Chaos Testing ‚≠ê (COMPLETE!)

### What Was Built

**Multi-Stage Dockerfile** (`deploy/docker/Dockerfile`):
- 3-stage build: builder ‚Üí debug ‚Üí production
- Builder: Go 1.24, static binary with `CGO_ENABLED=0`
- Debug: Includes delve debugger for troubleshooting
- Production: Distroless base (~15MB final image)
- Exposes ports: 8080 (HTTP), 9000 (gRPC/metrics), 7000 (Raft)
- Non-root user (65532) for security

**Docker Compose 3-Node Cluster** (`deploy/docker/docker-compose.yaml`):
- 3 GoQueue broker nodes with health checks
- Prometheus for metrics scraping
- Grafana for visualization (port 3001)
- Traefik as reverse proxy/load balancer
- Resource limits (512MB memory per broker)
- Persistent volumes for each node

**Helm Chart** (`deploy/kubernetes/helm/goqueue/`):
- Chart v0.1.0, appVersion 0.2.0
- StatefulSet with PVC templates for persistent storage
- Headless service for peer discovery
- ClusterIP service for client access
- Optional dependencies:
  - kube-prometheus-stack (monitoring)
  - traefik (ingress controller)
- Features:
  - ServiceMonitor for Prometheus scraping
  - PrometheusRule for alerting (HighLag, DiskPressure, Down)
  - PodDisruptionBudget (minAvailable: 2)
  - Ingress for HTTP + gRPC with TLS support
  - Grafana dashboard ConfigMap
  - Configurable resources, replicas, persistence

**Terraform Multi-Cloud Modules**:

**AWS Module** (`deploy/terraform/modules/aws/main.tf`):
- VPC with public/private subnets across 3 AZs
- EKS cluster with managed node groups
- gp3 StorageClass (encrypted, 3000 IOPS)
- EBS CSI driver with IRSA authentication
- ALB for ingress traffic
- Helm provider to deploy GoQueue chart
- ~400 lines, production-ready

**GCP Module** (`deploy/terraform/modules/gcp/main.tf`):
- VPC with secondary ranges for pods/services
- GKE cluster with Workload Identity
- premium-rwo StorageClass (SSD)
- Node pool with autoscaling (1-10 nodes)
- Private cluster with NAT gateway
- ~400 lines, production-ready

**Azure Module** (`deploy/terraform/modules/azure/main.tf`):
- Resource group in specified region
- VNET with subnet for AKS
- AKS cluster with managed identity
- managed-premium StorageClass
- System-assigned identity for Azure integrations
- ~300 lines, production-ready

**Environment Configs** (`deploy/terraform/environments/`):
- `dev/main.tf`: Small instances, 2 replicas, 10Gi storage
- `prod/main.tf`: Large instances, 3 replicas, 100Gi storage

**Chaos Test Scripts** (`deploy/testing/chaos/`):
- `pod-kill.sh`: Random pod deletion, monitors recovery
- `node-drain.sh`: Cordon/drain node, verify PDB respected
- `network-partition.sh`: Uses Chaos Mesh NetworkChaos CRD
- `disk-pressure.sh`: Fill PVC to threshold, test backpressure
- `run-all.sh`: Run all chaos tests in sequence

**Load Test Scripts** (`deploy/testing/load/`):
- k6-based load testing framework
- `produce.js`: Producer throughput test
- `consume.js`: Consumer throughput test  
- `stress.js`: Combined stress test (15 min, ramping VUs)
- `run-load-tests.sh`: Test runner with quick/standard/stress modes
- Targets: 100K+ msg/s, p99 <10ms, <0.01% error rate

**Runbook Documentation** (`docs/runbook.md`):
- Quick reference with key metrics
- Deployment procedures (fresh install, upgrade, rollback)
- Day-to-day operations (health checks, topic/consumer management)
- Monitoring & alerting guide
- Troubleshooting guides
- Incident response procedures (P1/P2/P3)
- Maintenance procedures (PVC resize, node drain)
- Disaster recovery (backup/restore, cross-region failover)

### Architecture Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Container runtime | Distroless | Minimal attack surface, ~15MB image |
| K8s workload | StatefulSet | Stable network IDs, ordered deployment, persistent storage |
| Service discovery | Headless service | DNS-based peer discovery for Raft |
| Storage | PVC templates | Each pod gets dedicated persistent volume |
| Monitoring | ServiceMonitor | Native Prometheus Operator integration |
| Ingress | Optional Traefik | HTTP + gRPC support, TLS termination |
| Multi-cloud | Separate modules | Different APIs, avoid lowest common denominator |
| Chaos testing | Chaos Mesh + scripts | CRD-based, declarative, reproducible |
| Load testing | k6 | Modern, scriptable, good metrics |

### Directory Structure Created

```
deploy/
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                    # Multi-stage build
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yaml           # 3-node local cluster
‚îÇ   ‚îî‚îÄ‚îÄ config/
‚îÇ       ‚îú‚îÄ‚îÄ node-1.yaml
‚îÇ       ‚îú‚îÄ‚îÄ node-2.yaml
‚îÇ       ‚îî‚îÄ‚îÄ node-3.yaml
‚îú‚îÄ‚îÄ kubernetes/
‚îÇ   ‚îî‚îÄ‚îÄ helm/
‚îÇ       ‚îî‚îÄ‚îÄ goqueue/
‚îÇ           ‚îú‚îÄ‚îÄ Chart.yaml
‚îÇ           ‚îú‚îÄ‚îÄ values.yaml
‚îÇ           ‚îú‚îÄ‚îÄ README.md
‚îÇ           ‚îú‚îÄ‚îÄ .helmignore
‚îÇ           ‚îî‚îÄ‚îÄ templates/
‚îÇ               ‚îú‚îÄ‚îÄ _helpers.tpl
‚îÇ               ‚îú‚îÄ‚îÄ statefulset.yaml
‚îÇ               ‚îú‚îÄ‚îÄ service.yaml
‚îÇ               ‚îú‚îÄ‚îÄ configmap.yaml
‚îÇ               ‚îú‚îÄ‚îÄ serviceaccount.yaml
‚îÇ               ‚îú‚îÄ‚îÄ pdb.yaml
‚îÇ               ‚îú‚îÄ‚îÄ servicemonitor.yaml
‚îÇ               ‚îú‚îÄ‚îÄ prometheusrule.yaml
‚îÇ               ‚îú‚îÄ‚îÄ ingress.yaml
‚îÇ               ‚îî‚îÄ‚îÄ grafana-dashboards.yaml
‚îú‚îÄ‚îÄ terraform/
‚îÇ   ‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aws/main.tf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gcp/main.tf
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ azure/main.tf
‚îÇ   ‚îî‚îÄ‚îÄ environments/
‚îÇ       ‚îú‚îÄ‚îÄ dev/main.tf
‚îÇ       ‚îî‚îÄ‚îÄ prod/main.tf
‚îî‚îÄ‚îÄ testing/
    ‚îú‚îÄ‚îÄ chaos/
    ‚îÇ   ‚îú‚îÄ‚îÄ pod-kill.sh
    ‚îÇ   ‚îú‚îÄ‚îÄ node-drain.sh
    ‚îÇ   ‚îú‚îÄ‚îÄ network-partition.sh
    ‚îÇ   ‚îú‚îÄ‚îÄ disk-pressure.sh
    ‚îÇ   ‚îî‚îÄ‚îÄ run-all.sh
    ‚îî‚îÄ‚îÄ load/
        ‚îú‚îÄ‚îÄ produce.js
        ‚îú‚îÄ‚îÄ consume.js
        ‚îú‚îÄ‚îÄ stress.js
        ‚îî‚îÄ‚îÄ run-load-tests.sh
```

### Key Concepts Learned

**StatefulSet vs Deployment**:
- StatefulSet: Ordered pod creation, stable network IDs (goqueue-0, goqueue-1), PVC per pod
- Deployment: Random pod names, shared storage, stateless
- For message queues: Always StatefulSet (need stable identity for Raft, persistent storage)

**Headless Service**:
- `clusterIP: None` creates DNS records for each pod
- `goqueue-0.goqueue-headless.namespace.svc.cluster.local`
- Essential for Raft peer discovery

**PodDisruptionBudget**:
- Limits voluntary disruptions (upgrades, node drain)
- `minAvailable: 2` ensures majority for Raft quorum
- Kubernetes respects PDB during maintenance

**StorageClass per Cloud**:
- AWS: gp3 (newest, best price/performance)
- GCP: premium-rwo (regional SSD, auto-replication)
- Azure: managed-premium (SSD-backed)

**Chaos Engineering Categories**:
1. Pod failures (kill, OOM)
2. Node failures (drain, shutdown)
3. Network failures (partition, latency)
4. Storage failures (disk pressure, corruption)

---

## Up Next ‚Äì Milestone 16 (CLI Tool)

- **Objective**: Provide command-line interface for managing goqueue
- **Commands**:
  - `goqueue topic create/list/delete/describe`
  - `goqueue consumer-group list/describe/reset`
  - `goqueue publish <topic> <message>`
  - `goqueue consume <topic> [--group]`
  - `goqueue cluster status/nodes`
- **Features**:
  - Human-readable and JSON output formats
  - Shell completion
  - Configuration file support

## Milestone 15 - gRPC API & Go Client ‚≠ê (COMPLETE!)

### What Was Built

**gRPC Server** (`internal/grpc/`):
- Protocol Buffers v3 definitions with comprehensive service contracts
- `PublishService` - Single message and streaming publish
- `ConsumeService` - Server streaming for continuous message delivery
- `AckService` - Message acknowledgment (ack, nack, reject, extend visibility)
- `OffsetService` - Consumer offset commit, fetch, and reset
- `HealthService` - Standard gRPC health checking
- Request logging interceptor for observability
- Integration with existing broker

**Go Client Library** (`pkg/client/`):
- Low-level `Client` - Direct gRPC wrapper with all operations
- High-level `Producer` - Async sending, batching, key-based partitioning
- High-level `Consumer` - Consumer group support, auto-ack, message channels
- Connection management with keepalives
- Retry logic with exponential backoff
- Comprehensive comments explaining patterns

**Integration**:
- gRPC server starts alongside HTTP server in main.go
- Port 9000 for gRPC, Port 8080 for HTTP
- Graceful shutdown handling

**Tests**:
- `internal/grpc/server_test.go` - 13 tests covering all services
- `pkg/client/client_test.go` - 14 tests for client library
- Integration tests for publish/consume flow
- Concurrent access tests

### Architecture Decisions

| Decision | Choice | Why |
|----------|--------|-----|
| Protocol | gRPC with HTTP/2 | Binary, streaming, multiplexed |
| Consume Pattern | Server streaming | Push-based, efficient |
| Error Handling | gRPC status codes | Standard, well-defined |
| Client Design | Low-level + High-level | Flexibility + convenience |
| Health Check | Standard protocol | Compatible with K8s, load balancers |

### Files Created

```
api/proto/
‚îú‚îÄ‚îÄ goqueue.proto          # Service definitions
‚îú‚îÄ‚îÄ buf.yaml               # buf configuration  
‚îú‚îÄ‚îÄ buf.gen.yaml           # Code generation config
‚îî‚îÄ‚îÄ gen/go/                # Generated Go code
    ‚îú‚îÄ‚îÄ goqueue.pb.go
    ‚îî‚îÄ‚îÄ goqueue_grpc.pb.go

internal/grpc/
‚îú‚îÄ‚îÄ server.go              # gRPC server setup
‚îú‚îÄ‚îÄ services.go            # Service implementations
‚îú‚îÄ‚îÄ publish_service.go     # Publish operations
‚îú‚îÄ‚îÄ consume_service.go     # Consume streaming
‚îú‚îÄ‚îÄ ack_service.go         # Acknowledgment operations
‚îú‚îÄ‚îÄ offset_service.go      # Offset management
‚îú‚îÄ‚îÄ health_service.go      # Health checking
‚îî‚îÄ‚îÄ server_test.go         # Server tests

pkg/client/
‚îú‚îÄ‚îÄ client.go              # Low-level gRPC client
‚îú‚îÄ‚îÄ producer.go            # High-level producer
‚îú‚îÄ‚îÄ consumer.go            # High-level consumer
‚îî‚îÄ‚îÄ client_test.go         # Client tests
```

## What Works

### Milestone 14 - Time Index, Snapshots & Log Compaction ‚≠ê (Partial Complete!)
- **Time Index** ‚úÖ:
  - Binary format mapping timestamp‚Üíoffset (16 bytes per entry)
  - 4KB granularity (same as offset index for consistency)
  - O(log n) binary search for time-based lookups
  - Segment methods: `ReadFromTimestamp()`, `ReadTimeRange()`, `GetFirstTimestamp()`, `GetLastTimestamp()`
  - Corruption recovery: automatic rebuild from segment data
  - Test coverage: 9 tests, all passing
- **Coordinator Snapshots** ‚úÖ:
  - Binary snapshot format (32-byte header + variable entries)
  - CRC32 validation for corruption detection
  - Snapshot triggers: 10K records OR 5 minutes (whichever first)
  - Keep last 3 snapshots, auto-cleanup of old ones
  - Supports group coordinator and transaction coordinator state
  - File format: `snapshot-{type}-{offset}-{timestamp}.bin`
  - `SnapshotWriter`, `SnapshotReader`, `SnapshotManager` with complete lifecycle
  - Test coverage: 9 tests, all passing
- **Log Compaction** üîÑ (Planned):
  - Copy-on-compact strategy documented
  - Dirty ratio trigger (50% duplicates)
  - Tombstone retention (24 hours)
  - Needs integration with Log API (current implementation accesses segments directly)
  - Recommended approach: Topic-level compaction with directory-level operations

### Milestone 13 - Online Partition Scaling + Coordinators on Internal Topic ‚úÖ ‚≠ê
- **Internal Topic Infrastructure**:
  - `__consumer_offsets` internal topic (50 partitions, replication factor 3)
  - Binary format for offset commits and group metadata
  - Hash-based group-to-partition mapping: `partition = murmur3(groupID) % 50`
  - `InternalTopicManager` for lifecycle management
- **Fault-Tolerant Coordinator**:
  - `GroupCoordinator` interface abstracting coordinator implementations
  - `PartitionBackedCoordinator` persisting state to `__consumer_offsets`
  - `CoordinatorRouter` for hash-based routing to correct coordinator
  - Coordinator failover via ISR election (no bespoke election path)
- **Online Partition Scaling**:
  - Kafka-style scaling (add only, never reduce)
  - `PartitionScaler` with validation, assignment, and notification
  - Round-robin replica assignment across cluster nodes
  - Consumer group notification for automatic rebalance
- **Partition Reassignment**:
  - 3-phase workflow: expand replicas ‚Üí wait catchup ‚Üí shrink
  - Throttling support to control data movement rate
  - Progress tracking with completion percentage
  - Safety: ISR quorum checks before moves
- **Admin API** (HTTP endpoints):
  - `POST /admin/topics/{name}/partitions` - Add partitions
  - `POST /admin/reassignment` - Start partition reassignment
  - `GET /admin/reassignment/{id}` - Get reassignment progress
  - `DELETE /admin/reassignment/{id}` - Cancel reassignment
  - `GET /admin/scaling/{topic}` - Get scaling status

### Milestone 12 - Cooperative Rebalancing ‚úÖ ‚≠ê
- **Binary message encoding** with CRC32 Castagnoli checksums
- **Append-only log** with automatic segment rollover at 64MB
- **Segment files** (.log) with sealed/active states
- **Sparse index files** (.index) with 4KB granularity
- **Log reader** with offset-based consumption
- **Partition abstraction** wrapping the log
- **Topic management** with single partition support
- **Broker API** for publish/consume operations
- **Demo application** at cmd/goqueue/main.go

### Milestone 2 - Topics, Partitions & Producer API ‚úÖ
- **Murmur3 hash partitioner** for consistent key-based routing
- **Round-robin partitioner** for nil keys (even distribution)
- **Manual partitioner** for explicit partition selection
- **Multi-partition topics** with default 3 partitions
- **Client-side Producer** with background accumulator goroutine
- **Three batch triggers**: size (100), linger (5ms), bytes (64KB)
- **AckMode enum**: None/Leader/All (Leader/All same until M11)
- **HTTP REST API** with full CRUD for topics and publish/consume
- **Priority draining** in accumulator to prevent race conditions

### Milestone 3 - Consumer Groups & Offset Management ‚úÖ
- **Consumer group membership** with heartbeat-based session management
- **Range partition assignment** (Kafka-compatible strategy)
- **Generation ID tracking** for zombie consumer protection
- **Stop-the-world rebalancing** (eager protocol)
- **File-based offset storage** with JSON persistence
- **Auto-commit support** (5s interval) with manual commit API
- **Group coordinator** managing all consumer groups centrally
- **Session monitoring** with 30s timeout, 3s heartbeat interval
- **Chi router HTTP API** with middleware logging
- **Long-polling** for message consumption (30s default timeout)

### Milestone 4 - Reliability: ACKs, Visibility & DLQ ‚úÖ
- **Hybrid ACK model** - Per-message ACK combined with offset-based commits (best of SQS + Kafka)
- **Visibility timeout** - 30s default, configurable per-message, with heap-based tracking
- **Receipt handles** - Unique per-delivery (`topic:partition:offset:deliveryCount:nonce`)
- **Dead Letter Queue** - Per-topic DLQ with `.dlq` suffix, auto-creation, metadata preservation
- **Retry with exponential backoff** - Base 1s, multiplier 2x, max 60s
- **Max retries** - 3 attempts before DLQ routing
- **Extend visibility API** - For long-running processing tasks
- **ACK/NACK/Reject semantics**:
  - ACK: Success, message deleted, offset may advance
  - NACK: Transient failure, retry with backoff
  - Reject: Permanent failure, immediate DLQ
- **HTTP API endpoints**:
  - `POST /messages/ack` - Acknowledge successful processing
  - `POST /messages/nack` - Signal retry needed
  - `POST /messages/reject` - Send to DLQ (poison message)
  - `POST /messages/visibility` - Extend visibility timeout
  - `GET /reliability/stats` - ACK manager, visibility tracker, DLQ stats
- **Files created**:
  - `internal/broker/inflight.go` - InFlightMessage, ReceiptHandle, DLQMessage, ReliabilityConfig
  - `internal/broker/visibility_tracker.go` - Min-heap based timeout tracking
  - `internal/broker/ack_manager.go` - Per-message ACK state, retry queue
  - `internal/broker/dlq.go` - DLQ routing with auto-topic creation

### Milestone 5 - Native Delay & Scheduled Messages ‚úÖ
- **Hierarchical Timer Wheel** - 4-level wheel for O(1) timer operations
  - Level 0: 256 buckets √ó 10ms = 2.56 seconds
  - Level 1: 64 buckets √ó 2.56s = 2.73 minutes  
  - Level 2: 64 buckets √ó 2.73m = 2.91 hours
  - Level 3: 64 buckets √ó 2.91h = 7.76 days (max delay)
- **Delay Index** - Persistent storage for crash recovery
  - Binary format: 16-byte header + 32-byte entries
  - States: PENDING, DELIVERED, CANCELLED, EXPIRED
  - Per-topic index files in data/delay/{topic}/
- **Scheduler** - Coordinator connecting timer wheel, delay index, and broker
  - Schedule(topic, partition, offset, delay) - relative delay
  - ScheduleAt(topic, partition, offset, deliverAt) - absolute time
  - Cancel(topic, partition, offset) - cancel pending delivery
- **Broker Integration**:
  - `PublishWithDelay(topic, key, value, delay)` - relative delay
  - `PublishAt(topic, key, value, deliverAt)` - absolute timestamp
  - `CancelDelayed(topic, partition, offset)` - cancel pending
  - `GetDelayedMessages(topic, limit, skip)` - list pending
  - `DelayStats()` - scheduler statistics
- **HTTP API endpoints**:
  - `POST /topics/{name}/messages` - supports `delay` and `deliverAt` params
  - `GET /topics/{name}/delayed` - list pending delayed messages
  - `DELETE /topics/{name}/delayed/{partition}/{offset}` - cancel delayed
  - `GET /delay/stats` - scheduler and timer wheel statistics
- **Files created**:
  - `internal/broker/timer_wheel.go` - Hierarchical timer wheel implementation
  - `internal/broker/delay_index.go` - Persistent delay tracking
  - `internal/broker/scheduler.go` - Scheduler coordination
  - `internal/broker/timer_wheel_test.go` - Timer wheel tests
  - `internal/broker/delay_index_test.go` - Delay index tests
  - `internal/broker/scheduler_test.go` - Scheduler tests
- **Key Design Decisions**:
  - Messages written to log immediately (durability first)
  - Timer wheel tracks visibility, index tracks state
  - Zero/past delays fire immediately
  - Bucket position is authoritative (not DeliverAt time check)

### Milestone 6 - Priority Lanes ‚úÖ ‚≠ê
- **5 Priority Levels** - Critical(0), High(1), Normal(2), Low(3), Background(4)
  - Critical: Emergencies, circuit breakers (50% share)
  - High: Paid users, real-time updates (25% share)
  - Normal: Default traffic (15% share)
  - Low: Batch jobs, reports (7% share)
  - Background: Analytics, cleanup tasks (3% share)
- **32-byte Message Header** - Priority stored at position [24]
  - Format: Magic(2) + Version(1) + Flags(1) + CRC(4) + Offset(8) + Timestamp(8) + Priority(1) + Reserved(1) + KeyLen(2) + ValueLen(4)
  - Version: 1 (simplified, no V1/V2 branching during development)
- **Weighted Fair Queuing (WFQ)** - Deficit Round Robin algorithm
  - Weights: [50, 25, 15, 7, 3] for priorities 0-4
  - Each priority gets proportional share, not strict ordering
  - Critical always checked first each round
  - Deficit counter tracks fairness across rounds
- **Starvation Prevention** - 30s default timeout
  - If any priority hasn't been served for 30s, it gets boosted
  - Prevents low-priority messages from waiting forever
  - Configurable per scheduler
- **Per-Priority-Per-Partition Metrics (PPPP)**:
  - Ready count per priority
  - In-flight count per priority
  - Enqueue/dequeue rates per priority
  - Last served timestamps per priority
- **Broker Integration**:
  - `PublishWithPriority(topic, key, value, priority)` - publish with priority
  - Priority-aware consume respects WFQ ordering
  - `PriorityStats()` - per-priority-per-partition metrics
- **HTTP API endpoints**:
  - `POST /topics/{name}/messages` - supports `priority` param
  - `GET /topics/{name}/messages` - includes priority in response
  - `GET /priority/stats` - per-priority-per-partition metrics
- **Files created/modified**:
  - `internal/storage/priority.go` - Priority type, constants, validation
  - `internal/storage/message.go` - 32-byte header with Priority field
  - `internal/broker/priority_scheduler.go` - WFQ scheduler using DRR
  - `internal/broker/priority_index.go` - Per-partition priority tracking
  - `internal/broker/priority_scheduler_test.go` - 12 comprehensive tests
  - `internal/storage/segment.go` - Simplified reading (32-byte always)
- **Key Design Decisions**:
  - WFQ over strict priority (fairness > starvation risk)
  - DRR algorithm: O(1) dequeue, simple implementation
  - Priority persisted in message (survives restart)
  - Simplified versioning (V1 only during dev phase)
- **Bugs Fixed**:
  - uint8 underflow in loops (0 decrement ‚Üí 255)
  - Segment reading always uses 32-byte header

### Milestone 7 - Message Tracing ‚úÖ ‚≠ê
- **W3C Trace Context Format** - Industry standard trace propagation
  - TraceID: 16 bytes (128 bits), hex encoded to 32 chars
  - SpanID: 8 bytes (64 bits), hex encoded to 16 chars
  - Traceparent format: `00-{trace_id}-{span_id}-{flags}`
  - Example: `00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01`
- **34-byte Message Header** - Added HeaderLen field for trace context
  - Format: Magic(2) + Version(1) + Flags(1) + CRC(4) + Offset(8) + Timestamp(8) + Priority(1) + Reserved(1) + KeyLen(2) + ValueLen(4) + HeaderLen(2)
  - Headers encoded as: Count(2) + [KeyLen(2) + Key + ValLen(2) + Val] √ó N
  - Enables trace context propagation via `traceparent` header
- **Span Event Types** - Complete message lifecycle coverage
  - Publish: `publish.received`, `publish.partitioned`, `publish.persisted`
  - Consume: `consume.fetched`, `consume.acked`, `consume.nacked`, `consume.rejected`
  - Delay: `delay.scheduled`, `delay.ready`, `delay.cancelled`, `delay.expired`
  - Visibility: `visibility.timeout`, `visibility.extended`
  - DLQ: `dlq.routed`
- **Ring Buffer Storage** - In-memory fast access
  - Configurable capacity (default 10,000 spans)
  - Thread-safe with RWMutex
  - Methods: Push(), Count(), GetRecent(), GetByTraceID(), GetByTimeRange()
  - Automatic eviction of oldest spans on overflow
- **File Exporter** - Persistent JSON traces
  - JSONL format (one JSON object per line)
  - File rotation by size (default 10MB)
  - Directory: data/traces/traces-{date}-{seq}.json
- **Stdout Exporter** - Real-time trace output for debugging
  - JSON format with pretty printing option
  - Configurable writer (default: os.Stdout)
- **OTLP Exporter** - OpenTelemetry Protocol support
  - HTTP transport with protobuf encoding
  - Batching for efficiency
  - Configurable endpoint and headers
- **Jaeger Exporter** - Jaeger Thrift UDP support
  - Direct UDP transport (agent mode)
  - Compact Thrift serialization
  - Configurable agent host/port
- **Query Interface**:
  - GetTrace(traceID) - Complete trace by ID
  - GetRecentTraces(limit) - Most recent traces
  - SearchTraces(query) - Filter by topic, partition, time range, status
  - Stats() - Buffer usage, exporter status, sampling rate
- **Trace Struct** - Complete trace view
  - TraceID, Topic, Partition, Offset
  - StartTime, EndTime, Duration
  - Status: completed, pending, error
  - Spans array sorted by timestamp
- **Broker Integration**:
  - Tracer initialized on broker startup
  - Publish operations record spans automatically
  - Consume operations record spans automatically
  - Ack/Nack/Reject operations record spans
  - Configurable per-topic tracing enable/disable
  - Sampling rate support (1.0 = 100%)
- **HTTP API endpoints**:
  - `GET /traces` - List recent traces (limit param)
  - `GET /traces/{traceID}` - Get specific trace with all spans
  - `GET /traces/search` - Search with filters (topic, partition, time, status)
  - `GET /traces/stats` - Tracer statistics
- **Files created**:
  - `internal/broker/tracer.go` - Complete tracing implementation (~1700 lines)
  - `internal/broker/tracer_test.go` - 22 comprehensive tests
  - `internal/storage/message.go` - Updated to 34-byte header with headers support
  - `internal/api/server.go` - Trace API endpoints
- **Key Design Decisions**:
  - W3C Trace Context for interoperability (not custom format)
  - Ring buffer for fast recent access (no DB dependency)
  - File export for persistence (simple, portable)
  - Sampling support for high-throughput scenarios
  - Span-per-event (not span-per-message) for granular visibility
  - Headers in message format (not out-of-band) for durability

### Milestone 8 - Schema Registry ‚úÖ
- **JSON Schema Validation** - Draft 7 compatible pure-Go validator
  - Type validation: string, integer, number, boolean, array, object, null
  - Property validation: properties, required, additionalProperties
  - Constraints: minimum/maximum, minLength/maxLength, pattern, enum
  - Array support: items, minItems, maxItems, uniqueItems
  - Nested objects with recursive validation
  - Local $ref support for schema composition
- **Compatibility Modes** - Safe schema evolution
  - BACKWARD (default): New schema reads old data (Confluent default)
  - FORWARD: Old schema reads new data
  - FULL: Both BACKWARD and FORWARD compatible
  - NONE: No compatibility checking (development/testing)
- **Subject Naming** - TopicNameStrategy
  - Subject = Topic name (1:1 mapping)
  - Simple and intuitive for most use cases
  - Future: RecordNameStrategy, TopicRecordNameStrategy
- **Versioning** - Sequential integers per subject
  - Global unique ID across all subjects
  - Version numbers sequential within each subject
  - Duplicate schema detection (returns existing)
- **Storage** - File-based JSON persistence
  - Directory: data/schemas/{subject}/
  - Files: v1.json, v2.json, config.json
  - Global: _ids.json (ID ‚Üí subject:version mapping)
  - Survives restarts with full recovery
- **Caching** - In-memory validator cache
  - Compiled schemas cached for fast validation
  - Lazy loading on first use
  - Configurable cache size (default 1000)
- **Broker Integration**:
  - Schema validation in publish path
  - Per-subject validation enable/disable
  - Validation failure returns 400 error
  - Schema ID passed via `schema-id` header
- **HTTP API endpoints** (Confluent-compatible):
  - `POST /schemas/subjects/{subject}/versions` - Register schema
  - `GET /schemas/subjects/{subject}/versions` - List versions
  - `GET /schemas/subjects/{subject}/versions/latest` - Get latest
  - `GET /schemas/subjects/{subject}/versions/{version}` - Get specific
  - `DELETE /schemas/subjects/{subject}/versions/{version}` - Delete version
  - `GET /schemas/subjects` - List subjects
  - `POST /schemas/subjects/{subject}` - Check schema exists
  - `DELETE /schemas/subjects/{subject}` - Delete subject
  - `GET /schemas/ids/{id}` - Get by global ID
  - `POST /schemas/compatibility/subjects/{subject}/versions/{version}` - Test compatibility
  - `GET /schemas/config` - Get global config
  - `PUT /schemas/config` - Set global config
  - `GET /schemas/config/{subject}` - Get subject config
  - `PUT /schemas/config/{subject}` - Set subject config
  - `GET /schemas/stats` - Registry statistics
- **Files created**:
  - `internal/broker/schema_registry.go` - Core registry (~1200 lines)
  - `internal/broker/json_schema_validator.go` - JSON Schema validator (~500 lines)
  - `internal/broker/schema_registry_test.go` - 30+ comprehensive tests
  - `internal/api/server.go` - Schema API endpoints (15 handlers)
- **Key Design Decisions**:
  - JSON Schema only (Protobuf deferred, noted for future)
  - File-based storage matches existing patterns
  - Broker-side validation (not client-side)
  - Confluent-compatible API for familiarity
  - Per-subject config overrides global config
  - Soft delete for version safety

### Milestone 9 - Transactional Publish ‚úÖ ‚≠ê
- **Idempotent Producers** - Exactly-once publish semantics
  - ProducerId (int64) + Epoch (int16) for identity
  - Per-partition sequence numbers for ordering
  - Sequence validation: rejects out-of-order, detects duplicates
  - Sliding deduplication window (default 5 sequences)
  - Epoch-based zombie fencing (re-init bumps epoch)
- **Transaction Coordinator** - Two-phase commit lifecycle
  - States: Empty ‚Üí Ongoing ‚Üí PrepareCommit/Abort ‚Üí CompleteCommit/Abort
  - Transaction ID generation with timestamp + random hex
  - Heartbeat support (same pattern as consumer groups)
  - Session timeout for dead producer detection
  - Transaction timeout for abandoned transactions
  - Background goroutines for timeout checking, snapshot taking
- **Transaction Log** - File-based WAL + Snapshots
  - WAL Record Types: init_producer, begin_txn, add_partition, prepare_commit/abort, complete_commit/abort, heartbeat, expire_producer, update_sequence
  - JSON-encoded WAL records (one per line)
  - Periodic snapshots for fast recovery
  - Storage: data/transactions/transactions.log + producer_state.json
- **Control Records** - Transaction markers in partition log
  - Flags byte in 34-byte message header (bits 3-4)
  - FlagControlRecord = 0x08 (bit 3)
  - FlagTransactionCommit = 0x10 (bit 4)
  - Commit: FlagControlRecord | FlagTransactionCommit
  - Abort: FlagControlRecord only
  - ControlRecordPayload: ProducerId, Epoch, TransactionalId
  - Written to ALL partitions in transaction
- **Broker Integration**:
  - TransactionCoordinator initialized on broker startup
  - WriteControlRecord method for commit/abort markers
  - PublishTransactional method with sequence validation
- **LSO (Last Stable Offset) - Read Committed Isolation** ‚≠ê
  - **UncommittedTracker** - Tracks offsets in uncommitted transactions
    - Per-topic map: `topic:partition ‚Üí Set[offsets]`
    - `TrackOffset(txnID, topic, partition, offset)` - Mark offset as uncommitted
    - `ClearTransaction(txnID)` - Returns `[]partitionOffset` for abort handling
    - Thread-safe with RWMutex
  - **AbortedTracker** - Permanently hides aborted message offsets
    - Per-partition map: `topic:partition ‚Üí Set[offsets]`
    - `MarkAborted(offsets)` - Mark offsets from aborted transaction
    - `IsAborted(topic, partition, offset)` - O(1) lookup
    - Messages from aborted transactions never become visible
  - **Control Record Filtering** - Commit/abort markers invisible to consumers
    - `IsControlRecord()` method checks FlagControlRecord bit
    - Control records excluded from priority index
    - All 4 consume methods filter control records first
  - **Consume Filtering Pipeline** - Order matters for correctness:
    1. Filter control records (IsControlRecord)
    2. Filter delayed messages (deliverAt > now)
    3. Filter uncommitted transactions (UncommittedTracker.IsUncommitted)
    4. Filter aborted transactions (AbortedTracker.IsAborted)
  - **Transaction Completion Flow**:
    - Commit: `ClearTransaction()` removes from uncommitted ‚Üí messages visible
    - Abort: `ClearTransaction()` returns offsets ‚Üí `MarkTransactionAborted()` ‚Üí messages permanently hidden
  - **Test Coverage**: 3 comprehensive tests
    - `TestBroker_UncommittedTransactionFiltering` - Uncommitted invisible, committed visible
    - `TestBroker_AbortedTransactionFiltering` - Aborted messages stay invisible forever
    - `TestBroker_MixedTransactionalAndNormalMessages` - Normal messages unaffected
  - **Known Limitation**: AbortedTracker is in-memory only; won't survive broker restart (acceptable for M9 scope)
  - GetTransactionCoordinator accessor for HTTP handlers
- **HTTP API endpoints** (9 endpoints):
  - `POST /producers/init` - Initialize producer (get PID + epoch)
  - `POST /producers/{producerId}/heartbeat` - Producer heartbeat
  - `POST /transactions/begin` - Begin transaction
  - `POST /transactions/publish` - Publish within transaction
  - `POST /transactions/add-partition` - Add partition to transaction
  - `POST /transactions/commit` - Commit transaction
  - `POST /transactions/abort` - Abort transaction
  - `GET /transactions` - List active transactions
  - `GET /transactions/stats` - Coordinator statistics
- **Files created**:
  - `internal/broker/idempotent_producer.go` - PID assignment, sequence tracking (~900 lines)
  - `internal/broker/transaction_log.go` - WAL + snapshot persistence (~650 lines)
  - `internal/broker/transaction_coordinator.go` - Transaction lifecycle (~850 lines)
  - `internal/broker/transaction_coordinator_test.go` - 22+ comprehensive tests
- **Files modified**:
  - `internal/storage/message.go` - Control record flags and payload
  - `internal/broker/broker.go` - TransactionCoordinator integration
  - `internal/api/server.go` - Transaction API endpoints (~600 lines)
- **Key Design Decisions**:
  - File-based transaction log (not internal topic like Kafka)
  - WAL + Snapshot pattern for efficient recovery
  - Epoch-based zombie fencing (not generation-based)
  - Heartbeat + timeout (same as consumer groups)
  - Flags byte for control records (not separate record type)
  - Per-partition sequence tracking (not per-topic)
  - Types match Kafka wire format (int64 PID, int16 epoch, int32 seq)

### Milestone 10 - Cluster Formation & Metadata ‚úÖ ‚≠ê
- **Static Peer Discovery** - Configured list of peer addresses
  - Peers defined in YAML config under `cluster.peers`
  - Each peer: `host:port` for cluster communication
  - Separate client address for external API access
  - No service discovery (simpler for initial cluster work)
- **Node Identity** - Each broker has unique identity
  - `NodeID` - Configurable, falls back to hostname
  - `NodeAddress` - Host + Port combination with parsing utility
  - `NodeInfo` - Complete snapshot: ID, addresses, status, role, version, tags
  - `NodeStatus` enum: Unknown ‚Üí Alive ‚Üí Suspect ‚Üí Dead ‚Üí Leaving
  - `NodeRole` enum: Follower | Controller
- **Membership Manager** - Tracks all cluster members
  - Thread-safe node registry with RWMutex
  - Event system: NodeJoined, NodeLeft, NodeDied, NodeSuspect, NodeRecovered, ControllerChanged
  - Listener pattern for components to react to membership changes
  - Persistence to `{dataDir}/cluster/state.json`
  - Quorum calculation based on configured cluster size
  - `AliveNodes()`, `AliveCount()`, `HasQuorum()` methods
- **Heartbeat-Based Failure Detection** - Health monitoring
  - Heartbeat interval: 3s (conservative default)
  - Suspect timeout: 6s (2x interval) - marks node as suspect
  - Dead timeout: 9s (3x interval) - marks node as dead
  - Background goroutine at heartbeat interval
  - `RecordHeartbeat(nodeID)` updates membership, recovers suspects
  - Status transitions: Unknown‚ÜíAlive‚ÜíSuspect‚ÜíDead
  - Integration with membership event system
- **Lease-Based Controller Election** - Single leader per cluster
  - `ControllerState` enum: Follower | Candidate | Leader
  - Epoch-based terms (monotonically increasing)
  - Vote request/response protocol:
    - Higher epoch gets vote
    - One vote per epoch per node
    - VotedFor + VotedEpoch tracking
  - Lease timeout: 15s (controller must renew)
  - Renewal interval: 5s (3 chances to renew per lease)
  - `TriggerElection()` when controller dies
  - `AcknowledgeController()` for followers to track controller
- **Cluster Metadata Store** - Topic and partition assignments
  - `TopicMeta` - Name, partition count, replication factor, config map
  - `PartitionAssignment` - Topic, partition ID, leader node, replicas list, ISR
  - `ClusterMeta` - Version counter, topics map, assignments map
  - CRUD operations: CreateTopic, DeleteTopic, GetTopic, ListTopics
  - Assignment management: SetAssignment, GetAssignment, RemoveAssignment
  - Listener pattern for metadata change notifications
  - Persistence to `{dataDir}/cluster/metadata.json`
  - `DefaultTopicConfig()` helper for sensible defaults
- **Inter-Node HTTP API** - Communication layer
  - **ClusterServer** (inbound handlers):
    - `POST /cluster/heartbeat` - Record heartbeat from peer
    - `POST /cluster/join` - Handle join request
    - `POST /cluster/leave` - Handle graceful leave
    - `GET /cluster/state` - Return cluster state snapshot
    - `POST /cluster/vote` - Handle vote request
    - `GET /cluster/metadata` - Return cluster metadata
    - `GET /cluster/health` - Health check endpoint
  - **ClusterClient** (outbound requests):
    - `SendHeartbeat(ctx, addr, req)` - Send heartbeat to peer
    - `RequestJoin(ctx, addr, req)` - Request to join cluster
    - `RequestLeave(ctx, addr, req)` - Request graceful leave
    - `RequestVote(ctx, addr, req)` - Request vote from peer
    - `FetchState(ctx, addr)` - Fetch state from peer
    - `PushMetadata(ctx, addr, meta)` - Push metadata to peer
    - `BroadcastHeartbeats(ctx, peers, req)` - Parallel heartbeat to all
  - HTTP client with 5s timeout, JSON encoding
- **Bootstrap & Lifecycle Coordination** - Orchestration
  - **Coordinator** - Composes all cluster components
  - `CoordinatorEvent` types:
    - Bootstrap: Started, Complete, Failed
    - Membership: JoinedCluster, LeftCluster
    - Leadership: BecameController, LostController
    - Quorum: QuorumLost, QuorumRestored
  - **Start(ctx)** bootstrap sequence:
    1. Load persisted state (state.json + metadata.json)
    2. Register self in membership
    3. Discover peers (request join from each)
    4. Start failure detector background loop
    5. Wait for quorum (with timeout)
    6. Start controller election
    7. Start heartbeat broadcasting
    8. Emit BootstrapComplete event
  - **Stop(ctx)** graceful shutdown:
    1. Stop heartbeat broadcasting
    2. Stop failure detector
    3. Request graceful leave from peers
    4. Remove self from membership
    5. Persist final state
- **Broker Integration** - Bridge cluster and broker
  - `clusterCoordinator` wrapper in broker package
  - Initialized when `BrokerConfig.ClusterEnabled = true`
  - 60s timeout for cluster bootstrap
  - 30s timeout for graceful cluster shutdown
  - `ClusterModeConfig` struct: ClusterAddress, ClientAddress, Peers, QuorumSize
  - Leadership queries: `IsLeaderFor()`, `GetLeader()`, `GetReplicas()`
  - Metadata operations: `CreateTopicMeta()`, `DeleteTopicMeta()` (controller-only)
  - HTTP route registration via `RegisterRoutes(mux)`
- **Wire Protocol Messages**:
  - `HeartbeatRequest/Response` - Node ID, timestamp, status
  - `JoinRequest/Response` - Node info, success flag, error message, cluster state
  - `LeaveRequest/Response` - Node ID, graceful flag
  - `StateSyncRequest/Response` - Version, full cluster state
  - `ControllerVoteRequest/Response` - Candidate, epoch, vote granted, voter ID
- **Files created** (8 new files):
  - `internal/cluster/types.go` - Core data structures (~400 lines)
  - `internal/cluster/node.go` - Local node identity (~80 lines)
  - `internal/cluster/membership.go` - Membership management (~350 lines)
  - `internal/cluster/failure_detector.go` - Health monitoring (~200 lines)
  - `internal/cluster/controller_elector.go` - Leader election (~250 lines)
  - `internal/cluster/metadata_store.go` - Metadata storage (~350 lines)
  - `internal/cluster/cluster_server.go` - HTTP API (~400 lines)
  - `internal/cluster/coordinator.go` - Bootstrap orchestration (~450 lines)
  - `internal/broker/cluster_integration.go` - Broker bridge (~200 lines)
- **Test files created** (3 files, 20+ tests):
  - `internal/cluster/types_test.go` - Types and parsing tests
  - `internal/cluster/membership_test.go` - Membership manager tests
  - `internal/cluster/failure_detector_test.go` - Failure detection + election tests
- **Files modified**:
  - `internal/broker/broker.go` - Cluster integration (~50 lines added)
- **Key Design Decisions**:
  - Static peer discovery (dynamic via gossip in future milestone)
  - Lease-based election (simpler than Raft consensus)
  - File-based persistence (matches existing patterns)
  - HTTP/JSON inter-node comms (reuse existing HTTP infra)
  - Conservative heartbeat timing (prioritize stability over speed)
  - Epoch-based leadership terms (prevents split-brain)
  - Quorum required for operations (majority of configured size)
  - Graceful leave broadcasts to all peers
  - Controller-only metadata writes (followers forward to controller)

### Technical Decisions Made
| Decision | Choice | Rationale |
|----------|--------|-----------|
| Segment size | 64MB | Good balance for durability/performance |
| Index granularity | 4KB | Matches typical filesystem block size |
| Fsync interval | 1000ms | Good durability without excessive I/O |
| File I/O | Buffered (os.File) | Simpler, portable, good performance |
| Checksum | CRC32 Castagnoli | Hardware acceleration, widely used |
| Magic bytes | 0x47 0x51 ("GQ") | Identifies goqueue files |
| Hash algorithm | Murmur3 | Industry standard, Kafka compatible |
| Default partitions | 3 | Good parallelism, expandable in M13 |
| Batching location | Client-side | Reduces broker load, Kafka pattern |
| LingerMs=0 | Immediate flush | Kafka semantics for low latency |
| HTTP API style | REST-ish | Simple, curl-friendly, JSON |
| HTTP router | chi v5 | Lightweight, idiomatic, stdlib compatible |
| Session timeout | 30s | Kafka default, good balance |
| Heartbeat interval | 3s | ~10 heartbeats per session (margin) |
| Partition strategy | Range | Simple, deterministic, Kafka default |
| Rebalance protocol | Eager (stop-world) | Simpler; cooperative in M12 |
| Offset storage | File-based JSON | Simple, debuggable; Kafka uses topics |
| Auto-commit interval | 5s | Kafka default |
| MemberID format | clientID-randomHex | Unique, traceable |
| Poll timeout | 30s | Standard long-poll duration |
| ACK model | Hybrid (per-msg + offset) | Best of SQS (per-msg) + Kafka (offset) |
| Visibility timeout | 30s | SQS default, good for most workloads |
| Max retries | 3 | Industry standard before DLQ |
| Backoff strategy | Exponential (1s base, 2x) | Standard retry pattern |
| DLQ naming | `{topic}.dlq` | Clear, discoverable |
| Receipt handle format | `topic:partition:offset:deliveryCount:nonce` | Parseable, debuggable |
| Visibility heap | Min-heap | O(1) peek, O(log n) operations |
| Timer algorithm | Hierarchical wheel | O(1) operations, ~7.76 day max |
| Tick interval | 10ms | Good precision, low CPU overhead |
| Timer wheel levels | 4 (256/64/64/64) | Balance of granularity and range |
| Delay storage | Separate index file | Clean separation, efficient recovery |
| Delay API | Both relative and absolute | Maximum flexibility for producers |
| Max delay | ~7.76 days | Practical limit, fits 4-level wheel |
| Zero/past delay | Immediate fire | Intuitive behavior |
| Schema format | JSON Schema | Human readable, no codegen, simple |
| Schema storage | File-based JSON | Matches existing patterns, debuggable |
| Subject naming | TopicNameStrategy | Simple 1:1 mapping, intuitive |
| Compatibility default | BACKWARD | Confluent default, safest for teams |
| Schema ID location | Message header | Uses existing headers system |
| Validation point | Broker-side | Central enforcement, configurable |
| Versioning | Sequential integers | Simple, compact, industry standard |
| Validation failure | Reject with 400 | Fail fast, clear contract |
| Transaction log | File-based WAL + Snapshot | Matches goqueue patterns, simpler than topic |
| Zombie fencing | Epoch-based | Kafka-compatible, bumps on re-init |
| Producer ID type | int64 | Kafka wire format compatibility |
| Sequence tracking | Per-partition | Better parallelism than per-topic |
| Transaction timeout | 60s | Kafka default, allows slow consumers |
| Producer heartbeat | 3s | Same as consumer groups |
| Producer session | 30s | Same as consumer groups |
| Control records | Flags byte | Reuses existing header format |
| Dedup window | 5 sequences | Balance memory vs. retry coverage |
| LSO tracking | Offset set per partition | O(1) lookup, no position scanning like Kafka |
| Abort persistence | In-memory only | Acceptable for M9 scope; persistence in future |
| Control record hiding | All 4 consume methods | Consistent filtering regardless of consume style |
| Peer discovery | Static config | Simple, explicit; gossip planned for later |
| Controller election | Lease-based | Simpler than Raft; single leader with 15s lease |
| Cluster heartbeat | 3s interval | Conservative; 6s suspect, 9s dead thresholds |
| Cluster metadata | File-based JSON | Matches existing patterns (state.json + metadata.json) |
| Inter-node comms | HTTP/JSON | Reuse existing HTTP infrastructure |
| Node ID | Configurable + hostname fallback | Explicit control with sensible default |
| Quorum size | Configurable (default: majority) | Flexible for different deployment sizes |
| Cluster state persistence | JSON at dataDir/cluster/ | Debuggable, easy recovery inspection |
| Controller lease timeout | 15s | 3x renewal interval (5s), tolerates network blips |
| Epoch tracking | Per-elector monotonic | Prevents split-brain, enables vote validation |

## What's Left to Build

### Core (Must Have)
- [x] Append-only log with segments ‚úÖ
- [x] Offset indexes ‚úÖ
- [ ] Time indexes (deferred - low priority)
- [x] Multi-partition topics ‚úÖ
- [x] Producer with batching ‚úÖ
- [x] HTTP API ‚úÖ
- [x] Consumer groups ‚úÖ (M3)
- [x] Offset management ‚úÖ (M3)
- [x] Per-message ACK ‚úÖ (M4)
- [x] Visibility timeout ‚úÖ (M4)
- [x] Dead letter queue ‚úÖ (M4)
- [x] Transactional publish ‚úÖ (M9)

### Differentiators (Key Features) ‚≠ê
- [x] Native delay messages (timer wheel) ‚úÖ (M5)
- [x] Priority lanes ‚úÖ (M6)
- [x] Message tracing ‚úÖ (M7)
- [x] Schema registry ‚úÖ (M8)
- [x] Transactional producers ‚úÖ (M9)
- [ ] Cooperative rebalancing (M12)

### Distribution (Multi-Node)
- [x] Cluster membership ‚úÖ (M10)
- [x] Leader election ‚úÖ (M10)
- [ ] Log replication (M11)

### Operations
- [ ] gRPC API
- [ ] CLI tool
- [ ] Prometheus metrics
- [ ] Kubernetes deployment

## Known Issues

### Visibility Tracking in Simple Consume (Deferred)

**Problem:** The simple consume endpoints (`GET /topics/{name}/partitions/{id}/messages/*`) don't track visibility timeout. They are read-only operations that simply fetch and filter messages from storage.

**Why This Matters:**
- Without visibility tracking, multiple consumers calling the simple consume API could get duplicate messages
- No receipt handles are generated (receipt handles are only for consumer groups)
- No ACK/NACK/Reject support (those operations require consumer group membership)

**Current Behavior:**
- Simple consume methods: Just read ‚Üí filter ‚Üí return (no state tracking)
- Consumer groups: Full lifecycle with ACK manager, visibility tracker, receipt handles

**Why Deferred:**
- Design decision: Keep simple consume truly simple for read-only use cases
- If you need reliability features (ACK, visibility, DLQ), use consumer groups
- Consumer groups already have full visibility tracking via ACK manager
- Simple consume is best suited for:
  - Debugging/inspection (manually looking at messages)
  - Replay/reprocessing (reading historical data)
  - Read-only analytics consumers
  - Testing and development

**When to Implement:**
- If users request visibility tracking for simple consume
- If we want to unify simple consume and consumer group consume patterns
- Consider: Would add complexity and state management to what's meant to be a simple API

**Related Code:**
- Consumer groups use: `internal/broker/ack_manager.go` + `visibility_tracker.go`
- Simple consume: `broker.Consume()`, `ConsumeByOffset()`, `ConsumeByPriority()`, `ConsumeByPriorityWFQ()`

## Lessons Learned

### Session 4 - Milestone 4
1. **Hybrid ACK model advantage** - Combining per-message ACK with offset commits gives both fine-grained control (retry single message) AND efficient progress tracking (offset-based recovery after crash).
2. **Min-heap for visibility tracking** - O(1) to check next expiring message, O(log n) for add/remove. Perfect fit for timeout-based tracking with potentially thousands of in-flight messages.
3. **Receipt handles encode location** - Including topic:partition:offset in receipt handle allows routing without database lookup. The nonce prevents replay attacks.
4. **DLQ auto-creation is essential** - Can't require users to pre-create DLQ topics. Auto-create with same partition count as original, with `.dlq` suffix for discoverability.
5. **Backpressure via MaxInFlightPerConsumer** - Without limits, a fast producer can exhaust memory. Default 1000 in-flight per consumer provides safe backpressure.

### Session 3 - Milestone 3
1. **Chi router URL params** - chi uses `chi.URLParam(r, "name")` to get URL params, not explicit arguments. Test code must call through `router.ServeHTTP()` not individual handlers.
2. **JSON partition keys as strings** - HTTP JSON APIs need partition IDs as strings (JSON keys), not ints. Convert with `strconv.Itoa()` for JSON serialization.
3. **Generation ID prevents zombies** - Stale consumers with old generation get rejected on heartbeat/commit. Critical for at-least-once guarantees.
4. **Stop-the-world rebalance tradeoff** - Simpler than incremental, but all consumers pause during rebalance. Good enough for v1; cooperative rebalancing in M12.

### Session 2 - Milestone 2
1. **Go's select is non-deterministic** - When multiple channels are ready, Go picks randomly. Fixed accumulator race condition by adding priority draining before flush/close.
2. **Pointer vs value semantics for channels** - `Send(record)` copies the struct, losing the `resultCh`. Fixed by creating `sendPtr(*ProducerRecord)` internal function.
3. **Test determinism with partitions** - Multi-partition defaults break tests that assume offset ordering. Solution: use single-partition configs in tests needing deterministic behavior.

## Performance Benchmarks

*No benchmarks yet - will add as milestones complete*

| Metric | Target | Current | Status |
|--------|--------|---------|--------|
| Write throughput | 100K msg/s | - | ‚è≥ |
| p99 publish latency | < 10ms | - | ‚è≥ |
| Rebalance time | < 1s | - | ‚è≥ |
