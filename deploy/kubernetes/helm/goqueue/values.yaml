# =============================================================================
# GOQUEUE HELM VALUES
# =============================================================================
#
# This file contains all configurable values for the GoQueue Helm chart.
# Override these values by:
#   1. Creating your own values.yaml: helm install -f my-values.yaml
#   2. Using --set: helm install --set replicaCount=5
#
# =============================================================================

# -----------------------------------------------------------------------------
# GLOBAL SETTINGS
# -----------------------------------------------------------------------------
global:
  # Image pull secrets (for private registries)
  imagePullSecrets: []
  
  # Storage class for PVCs (empty = cluster default)
  storageClass: ""

# -----------------------------------------------------------------------------
# DEPLOYMENT MODE
# -----------------------------------------------------------------------------
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ GoQueue supports three deployment modes:                                    │
# │                                                                             │
# │ 1. SINGLE (replicaCount: 1)                                                 │
# │    - One broker, no replication                                             │
# │    - Good for: Development, testing                                         │
# │    - NOT for production (single point of failure)                           │
# │                                                                             │
# │ 2. CLUSTER-LITE (replicaCount: 3)                                           │
# │    - 3 brokers with replication                                             │
# │    - Good for: Small production, startups                                   │
# │    - Tolerates 1 node failure                                               │
# │                                                                             │
# │ 3. PRODUCTION (replicaCount: 5+)                                            │
# │    - 5+ brokers with replication factor 3                                   │
# │    - Good for: Large production workloads                                   │
# │    - Tolerates 2 node failures                                              │
# └─────────────────────────────────────────────────────────────────────────────┘

# Number of GoQueue broker replicas
# For production: minimum 3 (odd number recommended for leader election)
replicaCount: 3

# -----------------------------------------------------------------------------
# IMAGE CONFIGURATION
# -----------------------------------------------------------------------------
image:
  repository: ghcr.io/abd-ulbasit/goqueue
  pullPolicy: IfNotPresent
  # Overrides the image tag (default is Chart appVersion)
  tag: "v0.7.0-security-schemas"

# Additional image pull secrets
imagePullSecrets: []

# Override chart name
nameOverride: ""
fullnameOverride: ""

# -----------------------------------------------------------------------------
# SERVICE ACCOUNT
# -----------------------------------------------------------------------------
serviceAccount:
  # Create a service account for GoQueue pods
  create: true
  
  # Annotations to add (useful for IAM roles on AWS/GCP)
  annotations: {}
  
  # Name override (auto-generated if empty)
  name: ""

# -----------------------------------------------------------------------------
# POD CONFIGURATION
# -----------------------------------------------------------------------------
podAnnotations:
  # Prometheus scraping (if using pod annotations instead of ServiceMonitor)
  prometheus.io/scrape: "true"
  prometheus.io/port: "8080"
  prometheus.io/path: "/metrics"

podLabels: {}

# Pod security context (applied to all containers in pod)
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 65532
  runAsGroup: 65532
  fsGroup: 65532
  fsGroupChangePolicy: "OnRootMismatch"

# Container security context
securityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  capabilities:
    drop:
      - ALL

# -----------------------------------------------------------------------------
# GOQUEUE CONFIGURATION
# -----------------------------------------------------------------------------
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ These settings are passed to GoQueue via ConfigMap or environment vars.    │
# │ They map to the goqueue config.yaml structure.                             │
# └─────────────────────────────────────────────────────────────────────────────┘

config:
  # Cluster settings
  cluster:
    enabled: true
    replicationFactor: 2
    minInSyncReplicas: 1
    replicationTimeout: "5s"
  
  # Storage settings
  storage:
    segmentSize: 1073741824  # 1GB
    indexInterval: 4096
    syncOnWrite: false
    syncInterval: "1s"
  
  # Default topic settings
  defaults:
    topic:
      partitions: 6
      retention:
        hours: 168  # 7 days
        bytes: -1   # unlimited
      delivery:
        visibilityTimeout: "30s"
        maxRetries: 3
  
  # Consumer settings
  consumer:
    sessionTimeout: "30s"
    heartbeatInterval: "10s"
    maxPollRecords: 500
    maxPollTimeout: "30s"
    autoCommitInterval: "5s"
  
  # Producer settings
  producer:
    batchSize: 16384
    lingerMs: 5
    acks: "leader"
    retries: 3
    retryBackoff: "100ms"
  
  # Logging
  logging:
    level: "info"
    format: "json"

# -----------------------------------------------------------------------------
# NETWORK / SERVICES
# -----------------------------------------------------------------------------
service:
  # Main service type (ClusterIP for internal, LoadBalancer for external)
  type: ClusterIP
  
  # HTTP API port
  httpPort: 8080
  
  # gRPC API port
  grpcPort: 9000
  
  # Internal cluster communication port
  internalPort: 7000
  
  # Annotations (for cloud load balancers)
  annotations: {}
  
  # External traffic policy (Cluster or Local)
  # Local preserves client IP but may have uneven distribution
  externalTrafficPolicy: Cluster

# Headless service for StatefulSet (used for internal discovery)
headlessService:
  annotations: {}

# -----------------------------------------------------------------------------
# INGRESS CONFIGURATION
# -----------------------------------------------------------------------------
ingress:
  enabled: false
  className: "traefik"
  annotations: {}
    # traefik.ingress.kubernetes.io/router.entrypoints: web
  hosts:
    - host: goqueue.local
      paths:
        - path: /
          pathType: Prefix
  tls: []
  #  - secretName: goqueue-tls
  #    hosts:
  #      - goqueue.local

# gRPC Ingress (separate because different protocol)
grpcIngress:
  enabled: false
  className: "traefik"
  annotations:
    traefik.ingress.kubernetes.io/router.entrypoints: grpc
  hosts:
    - host: grpc.goqueue.local
      paths:
        - path: /
          pathType: Prefix
  tls: []

# -----------------------------------------------------------------------------
# PERSISTENCE (PVC)
# -----------------------------------------------------------------------------
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ PERSISTENCE IS CRITICAL FOR MESSAGE QUEUES                                  │
# │                                                                             │
# │ Without persistence:                                                        │
# │   - Pod restart = ALL MESSAGES LOST                                         │
# │   - Never acceptable in production                                          │
# │                                                                             │
# │ With persistence (PVC):                                                     │
# │   - Pod restart = Messages survive (replayed from WAL)                      │
# │   - Pod migration = PVC reattaches to new pod                               │
# │                                                                             │
# │ WHY STATEFULSET?                                                            │
# │   Deployments: All pods share storage (not suitable for distributed state)  │
# │   StatefulSets: Each pod gets its own PVC (pod-0, pod-1, pod-2)            │
# │                                                                             │
# │ STORAGE CLASS CONSIDERATIONS:                                               │
# │   - AWS EBS gp3: Good balance of cost/performance                           │
# │   - GCP SSD PD: Low latency, higher cost                                    │
# │   - Azure Premium: Best for production                                      │
# │   - Local SSD: Fastest but not portable (node failure = data loss)          │
# └─────────────────────────────────────────────────────────────────────────────┘

persistence:
  enabled: true
  
  # Storage class (empty = cluster default)
  # Examples: gp3, premium-rwo, standard-rwo
  storageClass: ""
  
  # Create a custom StorageClass with specific settings
  # This is useful for setting reclaimPolicy: Delete to auto-cleanup volumes
  createStorageClass:
    enabled: false
    name: goqueue-storage
    provisioner: "ebs.csi.aws.com"
    parameters:
      type: gp3
      encrypted: "true"
    # IMPORTANT: Set to Delete to prevent orphaned EBS volumes on uninstall
    # Retain: Volume persists after PVC deletion (orphaned volumes accumulate charges)
    # Delete: Volume deleted when PVC is deleted (clean teardown)
    reclaimPolicy: Delete
    volumeBindingMode: WaitForFirstConsumer
    allowVolumeExpansion: true
  
  # Access mode
  # ReadWriteOnce: Can only be mounted by one pod (standard for StatefulSet)
  accessMode: ReadWriteOnce
  
  # Storage size per broker
  # Rule of thumb: 2-3x your expected data retention
  size: 10Gi
  
  # Annotations for the PVC
  annotations: {}
  
  # Selector for pre-provisioned PVs
  selector: {}
  
  # Existing claim to use (instead of creating new)
  existingClaim: ""

# -----------------------------------------------------------------------------
# RESOURCE LIMITS
# -----------------------------------------------------------------------------
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ RESOURCE MANAGEMENT IN KUBERNETES                                           │
# │                                                                             │
# │ requests: Minimum guaranteed resources                                      │
# │   - Scheduler uses this to place pod on node                                │
# │   - Pod always gets at least this much                                      │
# │                                                                             │
# │ limits: Maximum allowed resources                                           │
# │   - CPU: Pod is throttled if it exceeds                                     │
# │   - Memory: Pod is OOM-killed if it exceeds                                 │
# │                                                                             │
# │ C5.XLARGE NODE SPECS (4 vCPU, 8 GB RAM):                                    │
# │   - We run 1 goqueue pod per node                                           │
# │   - Reserve ~1 CPU for system + kubelet                                     │
# │   - Reserve ~1.5 GB for system + kubelet                                    │
# │   - Available for goqueue: ~3 CPU, ~6 GB RAM                                │
# │                                                                             │
# │ GOQUEUE MEMORY BREAKDOWN (estimated for high throughput):                   │
# │   - Write buffers: ~256 MB                                                  │
# │   - Index caches: ~512 MB                                                   │
# │   - In-flight tracking: ~256 MB                                             │
# │   - gRPC buffers: ~256 MB                                                   │
# │   - Go runtime/GC headroom: ~1 GB                                           │
# │   - Total recommended: 4 GB (with headroom)                                 │
# └─────────────────────────────────────────────────────────────────────────────┘

resources:
  limits:
    cpu: "3"
    memory: 5Gi
  requests:
    cpu: "2"
    memory: 4Gi

# -----------------------------------------------------------------------------
# AUTOSCALING
# -----------------------------------------------------------------------------
# Note: Message queues are stateful; horizontal scaling requires careful 
# partition management. Use with caution.
autoscaling:
  enabled: false
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

# -----------------------------------------------------------------------------
# NODE PLACEMENT
# -----------------------------------------------------------------------------
# Node selector (simple node selection)
nodeSelector: {}
  # kubernetes.io/arch: amd64
  # node-type: message-queue

# Tolerations (for taints)
tolerations: []
  # - key: "dedicated"
  #   operator: "Equal"
  #   value: "goqueue"
  #   effect: "NoSchedule"

# Affinity rules (advanced scheduling)
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ POD ANTI-AFFINITY - ONE POD PER NODE                                        │
# │                                                                             │
# │ For a distributed queue, we want each broker on a different node:           │
# │   - Node failure only affects one broker                                    │
# │   - Better resource utilization                                             │
# │   - Reduced blast radius                                                    │
# │                                                                             │
# │ requiredDuringSchedulingIgnoredDuringExecution = HARD requirement           │
# │   Kubernetes will NOT schedule if rule cannot be satisfied                  │
# │                                                                             │
# │ preferredDuringSchedulingIgnoredDuringExecution = SOFT preference           │
# │   Kubernetes will TRY to satisfy but schedule anyway if it cannot           │
# └─────────────────────────────────────────────────────────────────────────────┘
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
                - goqueue
        topologyKey: "kubernetes.io/hostname"

# Topology spread constraints (distribute across zones)
topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
      matchLabels:
        app.kubernetes.io/name: goqueue

# -----------------------------------------------------------------------------
# PROBES
# -----------------------------------------------------------------------------
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ KUBERNETES HEALTH PROBES                                                    │
# │                                                                             │
# │ livenessProbe: "Is the process stuck?"                                      │
# │   - Failure: Kubernetes restarts the container                              │
# │   - Use: Detect deadlocks, infinite loops                                   │
# │                                                                             │
# │ readinessProbe: "Can the pod serve traffic?"                                │
# │   - Failure: Pod removed from Service endpoints (no traffic)                │
# │   - Use: During startup, during graceful shutdown                           │
# │                                                                             │
# │ startupProbe: "Has the app finished starting?"                              │
# │   - Failure: Kubernetes restarts container                                  │
# │   - Use: Slow-starting apps (loading data, warming caches)                  │
# │   - While running: liveness/readiness probes are disabled                   │
# │                                                                             │
# │ GOQUEUE PROBE ENDPOINTS:                                                    │
# │   /health: Overall health (for liveness)                                    │
# │   /ready: Ready to serve traffic (for readiness)                            │
# │   /live: Simple alive check (for startup)                                   │
# └─────────────────────────────────────────────────────────────────────────────┘

livenessProbe:
  httpGet:
    path: /healthz
    port: http
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /readyz
    port: http
  initialDelaySeconds: 10
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 3

startupProbe:
  httpGet:
    path: /healthz
    port: http
  initialDelaySeconds: 5
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 30  # 30 * 5s = 150s max startup time

# -----------------------------------------------------------------------------
# POD DISRUPTION BUDGET
# -----------------------------------------------------------------------------
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ PodDisruptionBudget protects against voluntary disruptions:                 │
# │   - Node drains (kubectl drain)                                             │
# │   - Cluster upgrades                                                        │
# │   - Cluster autoscaler scale-down                                           │
# │                                                                             │
# │ Does NOT protect against:                                                   │
# │   - Node crashes (involuntary)                                              │
# │   - Pod OOM kills                                                           │
# │                                                                             │
# │ For a 3-node cluster: minAvailable=2 ensures quorum during maintenance      │
# └─────────────────────────────────────────────────────────────────────────────┘

podDisruptionBudget:
  enabled: true
  minAvailable: 2
  # maxUnavailable: 1  # Alternative to minAvailable

# -----------------------------------------------------------------------------
# MONITORING
# -----------------------------------------------------------------------------
metrics:
  enabled: true
  
  # ServiceMonitor for Prometheus Operator
  serviceMonitor:
    enabled: true
    
    # Additional labels for ServiceMonitor discovery
    additionalLabels: {}
    
    # Namespace for ServiceMonitor (default: release namespace)
    namespace: ""
    
    # Scrape interval
    interval: 15s
    
    # Scrape timeout
    scrapeTimeout: 10s
    
    # Path to metrics endpoint
    path: /metrics
    
    # Metric relabeling (optional)
    metricRelabelings: []
    
    # Target relabeling (optional)
    relabelings: []

  # PrometheusRule for alerts
  # ============================================================================
  # ALERT RULES OVERVIEW
  # ============================================================================
  # Each alert includes:
  #   - expr: PromQL expression that triggers the alert
  #   - for: Duration condition must be true before firing
  #   - severity: critical (PagerDuty) or warning (Slack only)
  #   - runbook_url: Link to troubleshooting guide
  #
  # SEVERITY LEVELS:
  #   critical - Pages on-call, requires immediate action
  #   warning  - Slack notification, investigate during business hours
  # ============================================================================
  prometheusRule:
    enabled: true
    additionalLabels: {}
    namespace: ""
    # Base URL for runbooks (customize to your docs site)
    runbookBaseUrl: "https://goqueue.dev/docs/runbooks"
    rules:
      # ======================================================================
      # CONSUMER ALERTS
      # ======================================================================
      - alert: GoQueueHighConsumerLag
        expr: sum(goqueue_consumer_lag) by (consumer_group, topic) > 10000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High consumer lag detected"
          description: "Consumer group {{ $labels.consumer_group }} has {{ $value | printf \"%.0f\" }} messages lag on topic {{ $labels.topic }}"
          runbook_url: "https://goqueue.dev/docs/runbooks#goqueuehighconsumerlag"
      
      - alert: GoQueueCriticalConsumerLag
        expr: sum(goqueue_consumer_lag) by (consumer_group, topic) > 100000
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "CRITICAL: Consumer lag exceeds 100k messages"
          description: "Consumer group {{ $labels.consumer_group }} has {{ $value | printf \"%.0f\" }} messages lag on topic {{ $labels.topic }}. Immediate action required."
          runbook_url: "https://goqueue.dev/docs/runbooks#goqueuecriticalconsumerlag"
      
      - alert: GoQueueNoOffsetCommits
        expr: sum by (consumer_group) (increase(goqueue_consumer_offset_commits_total[10m])) == 0
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "No offset commits from consumer group"
          description: "Consumer group {{ $labels.consumer_group }} has not committed any offsets in 10 minutes. Consumers may be dead or stuck."
          runbook_url: "https://goqueue.dev/docs/runbooks#goqueuenooffsetcommits"
      
      # ======================================================================
      # CLUSTER HEALTH ALERTS
      # ======================================================================
      - alert: GoQueueBrokerDown
        expr: up{job="goqueue"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "GoQueue broker is down"
          description: "GoQueue broker {{ $labels.instance }} has been down for more than 1 minute."
          runbook_url: "https://goqueue.dev/docs/runbooks#goqueuenodedown"
      
      - alert: GoQueueNodeDown
        expr: count(up{job="goqueue"} == 1) < 3
        for: 30s
        labels:
          severity: critical
        annotations:
          summary: "GoQueue cluster degraded"
          description: "Only {{ $value }} nodes are healthy. Cluster requires 3 nodes for full availability."
          runbook_url: "https://goqueue.dev/docs/runbooks#goqueuenodedown"
      
      - alert: GoQueueOfflinePartitions
        expr: goqueue_cluster_offline_partitions > 0
        for: 30s
        labels:
          severity: critical
        annotations:
          summary: "GoQueue has offline partitions"
          description: "{{ $value | printf \"%.0f\" }} partition(s) have no leader. DATA IS UNAVAILABLE."
          runbook_url: "https://goqueue.dev/docs/runbooks#goqueueofflinepartitions"
      
      - alert: GoQueueUnderReplicatedPartitions
        expr: goqueue_cluster_under_replicated_partitions > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Under-replicated partitions detected"
          description: "{{ $value | printf \"%.0f\" }} partition(s) have fewer replicas than configured. Data durability at risk."
          runbook_url: "https://goqueue.dev/docs/runbooks#goqueueunderreplicatedpartitions"
      
      # ======================================================================
      # PERFORMANCE ALERTS
      # ======================================================================
      - alert: GoQueueHighPublishLatency
        expr: histogram_quantile(0.99, sum(rate(goqueue_broker_publish_latency_seconds_bucket[5m])) by (le)) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High publish latency"
          description: "p99 publish latency is {{ $value | printf \"%.3f\" }}s (threshold: 100ms)"
          runbook_url: "https://goqueue.dev/docs/runbooks#goqueuehighpublishlatency"
      
      - alert: GoQueueHighErrorRate
        expr: sum(rate(goqueue_broker_messages_failed_total[5m])) / sum(rate(goqueue_broker_messages_published_total[5m])) > 0.01
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High message failure rate"
          description: "{{ $value | printf \"%.2f\" | mul 100 }}% of publish attempts are failing."
          runbook_url: "https://goqueue.dev/docs/runbooks#goqueuehigherrorrate"
      
      # ======================================================================
      # STORAGE ALERTS
      # ======================================================================
      - alert: GoQueueHighDiskUsage
        expr: (goqueue_storage_disk_used_bytes / goqueue_storage_disk_total_bytes) * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High disk usage on GoQueue"
          description: "Disk usage is {{ $value | printf \"%.1f\" }}% on {{ $labels.instance }}."
          runbook_url: "https://goqueue.dev/docs/runbooks#goqueuehighdiskspace"
      
      - alert: GoQueueCriticalDiskUsage
        expr: (goqueue_storage_disk_used_bytes / goqueue_storage_disk_total_bytes) * 100 > 90
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "CRITICAL: Disk usage above 90%"
          description: "Disk usage is {{ $value | printf \"%.1f\" }}% on {{ $labels.instance }}. Broker may stop accepting writes."
          runbook_url: "https://goqueue.dev/docs/runbooks#goqueuehighdiskspace"
      
      - alert: GoQueueFsyncErrors
        expr: increase(goqueue_storage_fsync_errors_total[5m]) > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "fsync errors detected"
          description: "{{ $value | printf \"%.0f\" }} fsync errors in the last 5 minutes. DATA DURABILITY AT RISK."
          runbook_url: "https://goqueue.dev/docs/runbooks#goqueuefsyncerrors"
      
      - alert: GoQueueHighFsyncLatency
        expr: histogram_quantile(0.99, sum(rate(goqueue_storage_fsync_latency_seconds_bucket[5m])) by (le)) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High fsync latency"
          description: "p99 fsync latency is {{ $value | printf \"%.3f\" }}s. Disk I/O may be saturated."
          runbook_url: "https://goqueue.dev/docs/runbooks#goqueuehighfsynclatency"
      
      # ======================================================================
      # REBALANCE ALERTS
      # ======================================================================
      - alert: GoQueueFrequentRebalances
        expr: sum by (consumer_group) (increase(goqueue_consumer_rebalances_total[5m])) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Frequent consumer rebalances"
          description: "Consumer group {{ $labels.consumer_group }} has rebalanced {{ $value | printf \"%.0f\" }} times in 5 minutes."
          runbook_url: "https://goqueue.dev/docs/runbooks#goqueuefrequentrebalances"
      
      # ======================================================================
      # SLO ALERTS (Error Budget)
      # ======================================================================
      - alert: GoQueueErrorBudgetBurn
        expr: (sum(rate(goqueue_broker_messages_failed_total[1h])) / sum(rate(goqueue_broker_messages_published_total[1h]))) / 0.001 > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Error budget burning too fast"
          description: "Error budget burn rate is {{ $value | printf \"%.1f\" }}x normal. At this rate, monthly budget will be exhausted."
          runbook_url: "https://goqueue.dev/docs/runbooks#goqueueerrorbudgetburn"

# Grafana dashboards
grafana:
  enabled: true
  dashboards:
    enabled: true
    # Label for dashboard discovery
    label: grafana_dashboard
    labelValue: "1"
    annotations: {}

# -----------------------------------------------------------------------------
# DISTRIBUTED TRACING CONFIGURATION (M25)
# -----------------------------------------------------------------------------
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ TRACING ARCHITECTURE                                                        │
# │                                                                             │
# │ GoQueue has built-in W3C Trace Context support. Traces flow through:        │
# │   1. Ring buffer (in-memory, fast lookup, lost on restart)                  │
# │   2. File export (persistent JSON on disk)                                  │
# │   3. OTLP export (production: Grafana Tempo, Jaeger)                        │
# │                                                                             │
# │ TRACE PROPAGATION:                                                          │
# │   Producer ──[traceparent header]──► Broker ──[header]──► Consumer          │
# │                                      │                                      │
# │                                      ▼                                      │
# │                                OTLP Exporter ──► Tempo/Jaeger               │
# │                                                                             │
# │ COMPARISON:                                                                 │
# │   - Kafka: External instrumentation (Zipkin/OTEL libraries)                │
# │   - RabbitMQ: rabbitmq_tracing plugin (logs to file)                        │
# │   - SQS: AWS X-Ray integration (proprietary)                               │
# │   - goqueue: Native W3C + OTLP export (open standard)                      │
# └─────────────────────────────────────────────────────────────────────────────┘

tracing:
  # Enable distributed tracing
  enabled: true

  # Sampling rate: 0.0 (no traces) to 1.0 (all traces)
  # Production recommendation: 0.1 (10%) to reduce storage and CPU overhead
  samplingRate: 1.0

  # Ring buffer capacity (number of spans kept in-memory)
  # Higher = more in-memory traces queryable via /traces API
  # Each span is ~100 bytes, so 100K spans ≈ 10MB
  ringBufferCapacity: 100000

  # File export (persistent JSON traces on disk)
  fileExport:
    enabled: true
    # Maximum file size before rotation (bytes)
    maxFileSize: 10485760  # 10MB

  # OTLP export (production: Grafana Tempo, Jaeger, Datadog, etc.)
  otlp:
    # Enable sending traces to an OTLP-compatible backend
    enabled: false

    # Endpoint for OTLP receiver
    # Tempo: tempo.monitoring:4317 (gRPC) or tempo.monitoring:4318 (HTTP)
    # Jaeger: jaeger-collector.monitoring:4317
    endpoint: ""

    # Protocol: "grpc" (recommended) or "http"
    protocol: "grpc"

    # Disable TLS for local/internal endpoints
    insecure: true

    # Optional: authentication headers
    # headers:
    #   Authorization: "Bearer <token>"
    headers: {}

    # Batch size (spans per batch before flush)
    batchSize: 512

    # Flush interval (how often to send batches)
    flushInterval: "5s"

    # Service name reported in traces
    serviceName: "goqueue"

# -----------------------------------------------------------------------------
# ALERTMANAGER CONFIGURATION (M24)
# -----------------------------------------------------------------------------
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ ALERTMANAGER ROUTING                                                        │
# │                                                                             │
# │ Configure how alerts are routed to notification channels:                   │
# │   - PagerDuty: Critical alerts, pages on-call engineer                      │
# │   - Slack: All alerts for visibility                                        │
# │   - Email: Digest or escalation                                             │
# │                                                                             │
# │ SEVERITY ROUTING:                                                           │
# │   critical → PagerDuty + Slack (immediate, pages on-call)                   │
# │   warning  → Slack only (business hours, non-urgent)                        │
# │                                                                             │
# │ COMPARISON:                                                                 │
# │   - Datadog: Notification channels per monitor                              │
# │   - PagerDuty: Escalation policies per service                              │
# │   - Prometheus: Route tree with matchers (this)                             │
# └─────────────────────────────────────────────────────────────────────────────┘

alertmanager:
  # Enable Alertmanager configuration (creates Secret + ConfigMap)
  enabled: true
  
  # Namespace for Alertmanager config (default: release namespace)
  namespace: ""
  
  # Label to associate config with Alertmanager instance
  # Must match your Alertmanager's alertmanagerConfigSelector
  alertmanagerLabel: "main"
  
  # Domain for dashboard/runbook links in alerts
  domain: "https://abd-ulbasit.github.io/goqueue"
  
  # Default receiver for unmatched alerts
  defaultReceiver: "slack-notifications"
  
  # Receiver for critical alerts (pages on-call)
  criticalReceiver: "pagerduty-critical"
  
  # Receiver for consumer lag alerts (data team)
  consumerLagReceiver: "slack-consumer-lag"
  
  # Receiver for storage/disk alerts (infra team)
  storageReceiver: "slack-infrastructure"
  
  # Alert grouping settings
  groupWait: "30s"        # Wait before sending first notification
  groupInterval: "5m"     # Wait between notifications for same group
  repeatInterval: "2h"    # Re-notify if alert still firing
  
  # Global settings
  global:
    # Slack webhook URL (get from: https://api.slack.com/messaging/webhooks)
    slackApiUrl: ""
    
    # PagerDuty Events API URL (usually default)
    pagerdutyUrl: "https://events.pagerduty.com/v2/enqueue"
    
    # SMTP settings for email alerts
    smtp:
      smarthost: ""       # e.g., smtp.gmail.com:587
      from: ""            # e.g., alerts@example.com
      username: ""
      password: ""
      requireTls: true
  
  # PagerDuty configuration
  pagerduty:
    enabled: false
    # Integration key from PagerDuty service
    # Create: Services → New Service → Prometheus integration
    routingKey: ""
  
  # Slack configuration
  slack:
    enabled: true
    # Default channel for all alerts
    channel: "#goqueue-alerts"
    # Channel for critical alerts (urgent)
    criticalChannel: "#goqueue-critical"
    # Channel for warning alerts
    warningChannel: "#goqueue-alerts"
    # Channel for consumer lag alerts (data platform team)
    consumerLagChannel: "#data-platform"
    # Channel for infrastructure alerts
    infrastructureChannel: "#infrastructure"
  
  # Email configuration
  email:
    enabled: false
    # Recipient email address
    to: ""
    # Recipient for critical alerts (can be different)
    criticalTo: ""
  
  # Webhook configuration (for custom integrations)
  webhook:
    enabled: false
    url: ""
    bearerToken: ""

# -----------------------------------------------------------------------------
# OPTIONAL DEPENDENCIES
# -----------------------------------------------------------------------------
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ These sub-charts are OPTIONAL. Enable for quick start, disable if you      │
# │ already have these running in your cluster.                                 │
# │                                                                             │
# │ DEFAULT: Enabled for "one-command" deployment experience                    │
# │ PRODUCTION: Usually disabled (use existing monitoring stack)                │
# └─────────────────────────────────────────────────────────────────────────────┘

# Prometheus stack (includes Grafana)
prometheus:
  enabled: true
  
  # Prometheus Operator configuration
  kube-prometheus-stack:
    prometheus:
      prometheusSpec:
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        ruleSelectorNilUsesHelmValues: false
    
    grafana:
      enabled: true
      adminPassword: "goqueue-admin"
      dashboardProviders:
        dashboardproviders.yaml:
          apiVersion: 1
          providers:
            - name: 'goqueue'
              orgId: 1
              folder: 'GoQueue'
              type: file
              disableDeletion: false
              editable: true
              options:
                path: /var/lib/grafana/dashboards/goqueue
    
    alertmanager:
      enabled: true

# Traefik ingress controller
traefik:
  enabled: true
  
  traefik:
    ports:
      web:
        port: 8000
        expose: true
        exposedPort: 80
      grpc:
        port: 9000
        expose: true
        exposedPort: 9000
    
    service:
      type: LoadBalancer
    
    logs:
      general:
        level: INFO
      access:
        enabled: true

# -----------------------------------------------------------------------------
# EXTRA OBJECTS
# -----------------------------------------------------------------------------
# Additional Kubernetes objects to create
extraObjects: []
  # - apiVersion: v1
  #   kind: ConfigMap
  #   metadata:
  #     name: extra-config
  #   data:
  #     key: value

# -----------------------------------------------------------------------------
# SECURITY CONFIGURATION (M21)
# -----------------------------------------------------------------------------
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ SECURITY FEATURES                                                           │
# │                                                                             │
# │ 1. TLS: Encrypt all HTTP/gRPC traffic                                       │
# │ 2. mTLS: Encrypt inter-node cluster communication                           │
# │ 3. API Key Auth: Require authentication for API access                      │
# │ 4. RBAC: Role-based access control for topics/groups                        │
# │                                                                             │
# │ COMPARISON:                                                                 │
# │   - Kafka: SASL/SCRAM + SSL + ACLs                                          │
# │   - RabbitMQ: TLS + Username/Password + vhost permissions                   │
# │   - goqueue: TLS + API Keys + RBAC                                          │
# └─────────────────────────────────────────────────────────────────────────────┘

security:
  # TLS for client connections (HTTP/gRPC APIs)
  tls:
    # Enable TLS for client-facing APIs
    enabled: true
    
    # Use auto-generated self-signed certificates (dev/testing only)
    selfSigned: true
    
    # Keep TLS secrets when Helm release is deleted (prevents cert loss)
    # Set to false for dev environments to allow clean teardown
    keepOnDelete: false
    
    # Use existing secret containing tls.crt and tls.key
    # Created via: kubectl create secret tls goqueue-tls --cert=cert.pem --key=key.pem
    existingSecret: ""
    
    # Minimum TLS version (1.2 or 1.3)
    minVersion: "1.2"
    
    # Certificate and key files (if not using existingSecret)
    certFile: ""
    keyFile: ""
    
    # CA file for client verification (optional, enables mTLS for clients)
    caFile: ""
  
  # mTLS for inter-node cluster communication
  clusterTls:
    # Enable mTLS for cluster communication
    enabled: true
    
    # Use auto-generated self-signed certificates (dev/testing only)
    selfSigned: true
    
    # Keep cluster TLS secrets when Helm release is deleted
    # Set to false for dev environments to allow clean teardown
    keepOnDelete: false
    
    # Use existing secret for cluster TLS
    existingSecret: ""
    
    # Require and verify client certificates (mTLS)
    requireClientCert: true

  # API Key authentication
  auth:
    # Enable API key authentication
    enabled: true
    
    # Allow unauthenticated access to health endpoints (/healthz, /readyz, /metrics)
    allowHealthWithoutAuth: true
    
    # Root API key for initial setup (should be rotated in production)
    # Set via: --set security.auth.rootKey=$(openssl rand -hex 32)
    rootKey: "3c78bc92f544273a7772dbcf5bbd8bfc872b103acbf5750ea2b4daf96330a1e2"
    
    # Use existing secret for root key
    # Secret should have key 'api-key'
    existingSecret: ""
  
  # Role-Based Access Control
  rbac:
    # Enable RBAC for topic/group access
    enabled: true
    
    # Default role for new API keys (admin, producer, consumer, readonly)
    defaultRole: "readonly"

# Secret for storing API keys (auto-created)
apiKeySecret:
  create: true
  name: goqueue-api-keys

# -----------------------------------------------------------------------------
# BACKUP & RESTORE CONFIGURATION (M23)
# -----------------------------------------------------------------------------
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ KUBERNETES-FIRST BACKUP STRATEGY                                            │
# │                                                                             │
# │ GoQueue uses a two-tier backup approach:                                    │
# │                                                                             │
# │ 1. VOLUME SNAPSHOTS (PVC-level)                                             │
# │    - Uses CSI VolumeSnapshots for consistent point-in-time backups          │
# │    - EBS snapshots on AWS, Persistent Disk snapshots on GCP                 │
# │    - Fast restore: Create new PVC from snapshot                             │
# │                                                                             │
# │ 2. APPLICATION-LEVEL EXPORT                                                 │
# │    - Topics, consumer groups, offsets, schemas exported to JSON             │
# │    - Uploaded to S3/GCS for cross-region disaster recovery                  │
# │    - Enables migration between clusters                                     │
# │                                                                             │
# │ COMPARISON:                                                                 │
# │   - Kafka: MirrorMaker 2 for cross-cluster replication                      │
# │   - RabbitMQ: Shovel/Federation + mnesia backup                             │
# │   - goqueue: VolumeSnapshot + JSON export                                   │
# └─────────────────────────────────────────────────────────────────────────────┘

backup:
  # Enable backup functionality
  enabled: true
  
  # Volume Snapshot Configuration
  volumeSnapshot:
    # Enable VolumeSnapshot-based backups
    enabled: true
    
    # VolumeSnapshotClass name (must exist in cluster)
    # AWS EBS: ebs-csi-snapclass (installed with EBS CSI driver)
    # GCP: standard-rwo-snapshot
    snapshotClassName: ""
    
    # Retention: number of snapshots to keep per PVC
    retentionCount: 7
  
  # Scheduled Backup (CronJob)
  schedule:
    # Enable scheduled backups
    enabled: true
    
    # Cron schedule (default: daily at 2 AM UTC)
    cron: "0 2 * * *"
    
    # Timezone for schedule
    timezone: "UTC"
    
    # Suspend scheduled backups (for maintenance)
    suspend: false
    
    # Number of successful jobs to keep
    successfulJobsHistoryLimit: 3
    
    # Number of failed jobs to keep
    failedJobsHistoryLimit: 1
  
  # Application-level backup to object storage
  objectStorage:
    # Enable S3/GCS backup
    enabled: false
    
    # Storage provider: s3, gcs, azure
    provider: "s3"
    
    # Bucket name
    bucket: ""
    
    # Bucket prefix/path
    prefix: "goqueue-backups"
    
    # Region (for S3)
    region: ""
    
    # Use existing secret for credentials
    # Secret should have: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY
    existingSecret: ""
  
  # Backup content selection
  include:
    # Include topic metadata and logs
    topics: true
    
    # Include consumer group offsets
    offsets: true
    
    # Include registered schemas
    schemas: true
    
    # Include configuration
    config: true

# -----------------------------------------------------------------------------
# RESTORE CONFIGURATION
# -----------------------------------------------------------------------------
restore:
  # Restore from a specific VolumeSnapshot
  # Set to snapshot name to trigger restore on next deployment
  fromSnapshot: ""
  
  # Restore from S3/GCS backup
  fromObjectStorage:
    # Enable restore from object storage
    enabled: false
    
    # Backup path to restore from (e.g., s3://bucket/goqueue-backups/2024-01-01-020000)
    path: ""
    
    # Skip topics that already exist
    skipExisting: true
